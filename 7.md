这份文档是同济大学倪张凯老师的《机器学习》第7章“支持向量机（SVM）”讲义，按文档顺序，核心内容可分为**基础概念、优化求解、扩展应用**三大模块，以下是逐部分详细拆解：


### 一、基础概念：二分类与最大间隔超平面
#### 1. 核心问题：二分类（Binary Classification）
SVM的核心目标是解决二分类问题——找到一个**超平面**将标签为 \( y=1 \)（正类）和 \( y=-1 \)（负类）的样本完全分离，且要求这个超平面具备三个关键特性：
- 容错性（tolerance）：对噪声样本不敏感；
- 鲁棒性（robustness）：小样本扰动不影响分类结果；
- 泛化能力（generalization）：对未见过的新样本分类效果好。

超平面的数学表达为 \( w^T x + b = 0 \)，其中：
- \( w \) 是超平面的法向量（决定超平面方向）；
- \( b \) 是偏置项（决定超平面位置）；
- \( x \) 是样本特征向量。

#### 2. 间隔（Margin）与支持向量（Support Vectors）
- **点到超平面的距离**：样本 \( x_i \) 到超平面的距离公式为 \( d = \frac{|w^T x_i + b|}{\|w\|} \)（\( \|w\| \) 是 \( w \) 的L2范数）；
- **间隔定义**：两类样本到超平面的“最小距离之和”称为间隔，即正类边界 \( w^T x + b = 1 \) 和负类边界 \( w^T x + b = -1 \) 之间的距离，计算得间隔为 \( \frac{2}{\|w\|} \)；
- **最大间隔准则**：SVM的核心是“最大化间隔”——间隔越大，泛化能力越强。而最大化 \( \frac{2}{\|w\|} \) 等价于**最小化 \( \frac{1}{2}\|w\|^2 \)**（简化计算，平方后可去掉根号，且与原目标等价）；
- **支持向量**：只有落在两个边界上的样本对超平面有影响，这些样本称为“支持向量”。其他样本无论位置如何，都不会改变超平面的参数，这是SVM的“稀疏性”核心。

#### 3. 硬间隔约束（Hard Margin Constraint）
要实现最大间隔分离，需满足约束条件：\( y_i(w^T x_i + b) \geq 1 \)（\( i=1,2,...,m \)，\( m \) 为样本数）。  
该约束的含义是：所有样本都必须落在两个边界之外（或边界上），此时问题转化为**凸二次规划（Convex Quadratic Programming）**——目标函数 \( \frac{1}{2}\|w\|^2 \) 是凸函数，约束是线性的，存在唯一最优解。


### 二、优化求解：拉格朗日乘数法、对偶问题与KKT条件
#### 1. 对偶问题（Dual Problem）引入
原始问题（最小化 \( \frac{1}{2}\|w\|^2 \) 并满足 \( y_i(w^T x_i + b) \geq 1 \)）直接求解复杂，SVM通过“对偶变换”将其转化为更易求解的对偶问题。  
文档中对“对偶问题”的定义：
- 对偶问题是原始优化问题的变换，可提供原始问题最优值的下界/上界，或简化求解；
- 核心工具是**拉格朗日函数（Lagrangian Function）**：将目标函数与约束条件结合，形式为 \( L(x, \lambda, v) = 目标函数 - \lambda^T \cdot (不等式约束) - \nu^T \cdot (等式约束) \)，其中 \( \lambda、\nu \) 是拉格朗日乘数（对偶变量）。

#### 2. 拉格朗日乘数法示例（辅助理解）
问题：找平面上距离原点最近、且在直线 \( y=2x+3 \) 上的点。  
求解步骤：
1. 目标函数（最小化距离平方，简化计算）：\( f(x,y) = x^2 + y^2 \)；
2. 约束条件：\( g(x,y) = y - 2x - 3 = 0 \)；
3. 构建拉格朗日函数：\( L(x,y,\lambda) = x^2 + y^2 + \lambda(y - 2x - 3) \)；
4. 求偏导并置为零：\( \frac{\partial L}{\partial x}=2x - 2\lambda=0 \)、\( \frac{\partial L}{\partial y}=2y + \lambda=0 \)、\( \frac{\partial L}{\partial \lambda}=y - 2x - 3=0 \)，联立求解得最优解。

#### 3. KKT条件示例（处理不等式约束）
问题：长方体体积最大，且长、宽、高之和 ≤ 10。  
求解步骤：
1. 目标函数（最大化体积）：\( f(x,y,z) = xyz \)；
2. 约束条件（不等式约束）：\( g(x,y,z) = x + y + z - 10 \leq 0 \)；
3. 构建拉格朗日函数：\( L(x,y,z,\lambda) = xyz - \lambda(x + y + z - 10) \)（\( \lambda \geq 0 \)）；
4. 应用KKT条件（不等式约束优化的必要条件）：
   - 梯度为零：\( \frac{\partial L}{\partial x}=yz - \lambda=0 \)、\( \frac{\partial L}{\partial y}=xz - \lambda=0 \)、\( \frac{\partial L}{\partial z}=xy - \lambda=0 \)、\( \frac{\partial L}{\partial \lambda}=-(x + y + z - 10)=0 \)；
   - 约束满足：\( x + y + z \leq 10 \)；
   - 乘数非负：\( \lambda \geq 0 \)；
   - 互补松弛性：\( \lambda(x + y + z - 10) = 0 \)（若约束严格满足，\( \lambda=0 \)；若 \( \lambda>0 \)，约束取等号）；
5. 联立求解：最优解为 \( x=y=z=\frac{10}{3} \)（体积最大）。

#### 4. 最优超平面的对偶求解（核心推导）
针对SVM的原始问题，通过拉格朗日乘数法和KKT条件转化为对偶问题，步骤如下：
1. 构建拉格朗日函数：  
   \( L(w,b,\alpha) = \frac{1}{2}\|w\|^2 + \sum_{i=1}^m \alpha_i(1 - y_i(w^T x_i + b)) \)，其中 \( \alpha_i \geq 0 \)（拉格朗日乘数）；
2. 求偏导并置为零（消去 \( w、b \)）：  
   - 对 \( w \) 求偏导：\( \frac{\partial L}{\partial w} = w - \sum_{i=1}^m \alpha_i y_i x_i = 0 \implies w = \sum_{i=1}^m \alpha_i y_i x_i \)；
   - 对 \( b \) 求偏导：\( \frac{\partial L}{\partial b} = -\sum_{i=1}^m \alpha_i y_i = 0 \implies \sum_{i=1}^m \alpha_i y_i = 0 \)；
3. 代入拉格朗日函数，转化为对偶问题：  
   原始的“最小化 \( \frac{1}{2}\|w\|^2 \)”转化为“最大化对偶目标函数”：  
   \( \max_\alpha \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j \)；  
   约束条件：\( \sum_{i=1}^m \alpha_i y_i = 0 \) 且 \( \alpha_i \geq 0 \)；
4. 求解对偶问题：  
   对偶问题是凸二次规划，常用 **SMO（Sequential Minimal Optimization）算法** 求解（每次优化两个 \( \alpha_i \)，降低计算复杂度），得到最优 \( \alpha^* \)；
5. 还原最优超平面参数：  
   - \( w^* = \sum_{i=1}^m \alpha_i^* y_i x_i \)（仅支持向量的 \( \alpha_i^* > 0 \)，非支持向量 \( \alpha_i^* = 0 \)）；
   - \( b^* \)：通过KKT条件的“互补松弛性”求解，取支持向量（\( \alpha_i^* > 0 \)）满足 \( y_i(w^{*T} x_i + b^*) = 1 \)，解得 \( b^* = y_i - w^{*T} x_i \)；
6. 最终分类函数：  
   \( f(x) = w^{*T} x + b^* = \sum_{i=1}^m \alpha_i^* y_i x_i^T x + b^* \)。

#### 5. SVM的KKT条件（最优解的判定）
对偶问题的最优解 \( \alpha^* \) 需满足：
- \( \alpha_i^* \geq 0 \)；
- \( y_i f(x_i) \geq 1 \)（原始约束）；
- \( \alpha_i^*(y_i f(x_i) - 1) = 0 \)（互补松弛性）。  
  推论：只有 \( y_i f(x_i) = 1 \) 的样本（支持向量）对应的 \( \alpha_i^* > 0 \)，其他样本 \( \alpha_i^* = 0 \)，即SVM的决策仅由支持向量决定。


### 三、扩展应用：软间隔、核技巧与模型延伸
#### 1. 软间隔（Soft Margin）：处理线性不可分与过拟合
硬间隔的前提是“样本线性可分”，但实际数据常含噪声或非线性，此时硬间隔无解或过拟合。软间隔的核心是“允许部分样本不满足约束”，通过引入**松弛变量 \( \varepsilon_i \geq 0 \)** 实现：
- 约束条件调整：\( y_i(w^T x_i + b) \geq 1 - \varepsilon_i \)（\( \varepsilon_i \) 表示样本偏离间隔边界的程度，\( \varepsilon_i=0 \) 对应硬间隔）；
- 目标函数调整：\( \min_{w,b,\varepsilon} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^m \varepsilon_i \)，其中 \( C > 0 \) 是惩罚系数：
  - \( C \) 越大：对违规样本（\( \varepsilon_i > 0 \)）的惩罚越重，模型越接近硬间隔，易过拟合；
  - \( C \) 越小：惩罚越轻，允许更多违规样本，模型泛化能力更强，但可能欠拟合。

#### 2. 软间隔的对偶与损失函数
- 对偶问题：与硬间隔类似，但 \( \alpha_i \) 的约束变为 \( 0 \leq \alpha_i \leq C \)（由拉格朗日乘数的关系 \( C = \alpha_i + \mu_i \)，\( \mu_i \geq 0 \) 推导）；
- 松弛变量与损失函数：\( \varepsilon_i = \max(0, 1 - y_i f(x_i)) \)，对应 **hinge loss（合页损失）**：  
  \( L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i)) \)。  
  文档对比了三种损失函数：
  - 合页损失：仅当样本在间隔内或误分类时（\( y_i f(x_i) < 1 \)）有损失，对异常值不敏感；
  - 平方损失：\( L = (y_i f(x_i) - 1)^2 \)，对异常值敏感；
  - 逻辑损失：光滑但尾部衰减慢，对异常值敏感。

#### 3. 核技巧（Kernel Trick）：解决非线性分类
当样本在原始空间线性不可分时，核心思路是：将样本映射到**高维特征空间 \( \phi(x) \)**，使样本在高维空间线性可分。但高维空间计算复杂度极高，核技巧的核心是“替代高维内积”，无需显式映射：
- 核函数定义：\( \kappa(x_i, x_j) = \phi(x_i)^T \phi(x_j) \)（替代高维空间的内积）；
- 核支持向量机的分类函数：\( f(x) = \sum_{i=1}^m \alpha_i^* y_i \kappa(x_i, x) + b^* \)；
- 常用核函数：
  1. 线性核：\( \kappa(x,z) = x^T z \)（对应原始空间的线性SVM）；
  2. 多项式核：\( \kappa(x,z) = (x^T z)^d \)（\( d \) 为次数，控制非线性程度）；
  3. 高斯核（RBF核）：\( \kappa(x,z) = \exp\left(-\frac{\|x-z\|^2}{2\sigma^2}\right) \)（\( \sigma \) 越小，模型越复杂，易过拟合）；
  4. Sigmoid核：\( \kappa(x,z) = \tanh(\beta x^T z + \theta) \)（模拟神经网络的激活函数）。

#### 4. 线性SVM的梯度与稀疏性
线性SVM的代价函数为“hinge loss + L2正则化”：\( \sum_{i=1}^m \max(0, 1 - y_i f(x_i)) + \lambda\|w\|^2 \)；
- 梯度计算：当 \( y_i f(x_i) < 1 \) 时，梯度为 \( -\sum y_i x_i \)；否则梯度为0；
- 稀疏性：仅支持向量（\( y_i f(x_i) = 1 \)）对梯度有贡献，非支持向量不影响参数更新，因此SVM是稀疏模型（与逻辑回归的全样本贡献不同）。

#### 5. 深度学习与SVM的对比
| 维度         | 深度学习                | 支持向量机（SVM）        |
|--------------|-------------------------|--------------------------|
| 特征变换     | 通过神经网络自动学习 \( \phi(x) \) | 通过核函数实现 \( \phi(x) \)（手动选择核函数） |
| 分类器       | 线性/非线性分类器       | 线性分类器（依赖核技巧） |
| 核心思路     | 端到端学习特征+分类     | 核函数映射+最大间隔分类  |
| 扩展         | 无核函数依赖            | 支持多核学习（Multiple Kernel Learning） |

#### 6. SVM工具与延伸方法
- 常用软件包：LIBSVM（通用SVM）、LIBLINEAR（线性SVM，效率高）、SVMlight、SVMstruct、Pegasos；
- 延伸方法：
  1. 支持向量回归（SVR）：将分类的间隔思想推广到回归问题；
  2. 排序SVM（Ranking SVM）：用于排序任务（如推荐系统）；
  3. 单类SVM（One-class SVM）：用于异常检测（仅需一类样本训练）。


### 总结
文档按“基础概念→优化求解→扩展应用”的逻辑展开，核心脉络是：  
**二分类问题→最大间隔超平面→硬间隔SVM（凸二次规划）→对偶问题（拉格朗日+KKT）→软间隔SVM（处理噪声）→核技巧（处理非线性）→线性SVM细节→工具与延伸**。  
SVM的核心优势是“最大间隔”带来的强泛化能力和“核技巧”带来的非线性处理能力，且通过支持向量实现稀疏性，计算效率较高。
# 同济大学《机器学习》第1章开学篇（按文档顺序详细总结）
## 一、课程基础信息（第1页）
- 授课院校：同济大学计算机科学与技术学院
- 授课教师：李洁、倪张凯
- 联系方式：nijanice@163.com（李洁）、zkni@tongji.edu.cn（倪张凯）
- 课程官网：https://eezkni.github.io/
- 文档日期：9/15/2025
- 章节主题：第1章 开学篇

## 二、核心概念界定（第2页）
### 1. 人工智能（Artificial Intelligence）
让计算机或机器能够执行通常需要人类智能才能完成的任务的整体技术和理论体系。

### 2. 机器学习（Machine Learning）
通过从数据中自动学习规律并改进性能，而无需显式编程的算法与方法。

### 3. 深度学习（Deep Learning）
使用深层架构的机器学习方法，其核心思想是通过多层次的非线性变换，从数据中自动提取更高层次的抽象特征。

### 4. 神经网络（Neural Networks）
最初指大脑神经元，人工智能受其启发，发展出人工神经网络。
- 关联关系：人工智能包含机器学习，机器学习包含深度学习，神经网络是深度学习的重要基础。

## 三、人工智能的方法（第3-5页）
人工智能方法分为两类，核心差异在于是否依赖显式规则或数据自主学习：

### 1. 基于规则（Rule-based）
- 实现方式：通过直接编程（Implemented by direct programing）
- 核心灵感：受人类启发式思维（Inspired by human heuristics）
- 特点：需人工定义明确规则，适用于逻辑清晰、场景固定的任务。

### 2. 基于数据（Data-based）
- 子类别1：专家系统（Expert systems）
  - 运作逻辑：专家或统计学家基于数据创建预测或决策规则（Experts or statisticians create rules based on data）
- 子类别2：机器学习（Machine Learning）
  - 运作逻辑：直接基于数据进行预测或决策，无需人工定义规则
  - 所属领域：数据科学（Data Science）
- 特点：无需显式编程，系统从数据中自主学习规律，适用于复杂、动态场景。

## 四、机器学习与传统编程的区别（第6页）
### 1. 传统编程（Traditional Programming）
- 流程：人类程序员（Human Programmer）→ 编写程序（Program）→ 输入数据（Input）→ 输出结果（Output）
- 核心：依赖人工定义完整规则，程序是“规则载体”。

### 2. 机器学习（Machine Learning）
- 流程：输入数据（Input Data）+ 算法（Algorithm）→ 自主学习程序（Learning Program）→ 输出结果（Output）
- 核心：无需人工定义显式规则，程序通过数据“自主习得”规律。

## 五、机器学习的适用场景（第7-8页）
机器学习适用于传统编程难以应对的场景，具体分为两类：

### 1. 传统编程无法或难以实现的场景
- 难以手动构建的系统：需特定细分技能或知识的任务（如语音识别、人脸识别）
- 程序复杂度极高的任务：机械臂控制、自动驾驶直升机、手写识别、多数自然语言处理任务、围棋博弈
- 无人类经验的场景：火星导航（navigating on Mars）
- 需快速决策的场景：高频交易（high-frequency trading）
- 解决方案随时间变化的场景：计算机网络路由（routing on a computer network）

### 2. 需自主适配或挖掘价值的场景
- 大规模个性化适配：个性化新闻/邮件过滤、个性化辅导、产品推荐
- 大数据挖掘新知识（数据挖掘）：购物篮分析（如网页点击数据）、医学文本挖掘（如偏头痛与钙通道阻滞剂、镁的关联）

## 六、机器学习的定义（第9-10页）
### 1. 权威定义（Herbert Simon）
- 学者背景：图灵奖（1975）、诺贝尔经济学奖（1978）得主，研究领域涵盖人工智能、人类认知心理学、经济组织决策过程
- 定义：学习是系统通过经验（experience）改进性能（performance）的任何过程（Learning is any process by which a system improves performance from experience）

### 2. 数学定义（Tom Mitchell）
- 核心要素：研究满足以下条件的算法——
  1. 改进性能（improve their performance P）
  2. 针对特定任务（at some task T）
  3. 基于经验（based on experience E）
  4. 无需显式编程（with non-explicit programming）
- 任务定义：一个明确的学习任务由三元组<P, T, E>表示

## 七、机器学习的学习条件（第11页）
当前机器学习快速发展的核心原因是“时机成熟”，满足三大条件：
1. 基础算法成熟：已有大量有效且高效的基础算法（Many basic effective and efficient algorithms available）
2. 数据资源充足：海量在线数据可获取（Large amounts of on-line data available）
3. 计算资源充沛：具备支撑大规模计算的硬件条件（Large amounts of computational resources available）
- 补充说明：模型参数量及计算量持续增大，例如万亿参数神经网络，模拟训练一次的电费约1000万美元

## 八、机器学习的学习任务与案例（第12-21页）
### 1. 核心学习任务类型
- 分类（Classification）：输出离散类别标签（如“垃圾邮件/非垃圾邮件”）
- 回归（Regression）：输出连续数值（如股票价格、蛋白质结构参数）
- 聚类（Clustering）：无标签数据的自主分组
- 降维（Dimensionality reduction）：简化数据维度同时保留核心信息

### 2. 典型案例解析
#### 案例1：手写数字识别（分类任务）
- 输入：28×28像素的图像，转化为784维向量\(x \in \mathbb{R}^{784}\)
- 目标：学习分类函数\(f(x)\)，实现\(f:x\to \{0,1,2,...,9\}\)（输出0-9的数字标签）

#### 案例2：人脸检测（监督分类任务）
- 分类目标：将图像窗口分为三类——非人脸（non-face）、正面脸（frontal-face）、侧脸（profile-face）
- 训练数据：5000张近正面脸（涵盖不同年龄、种族、性别、光照条件，已标准化处理）+ \(10^8\)张非人脸图像

#### 案例3：垃圾邮件检测（分类任务）
- 任务：将邮件分为“垃圾邮件/非垃圾邮件”
- 输入数据：词频特征（如“viagra”“outperform”等关键词的出现次数）
- 核心挑战：“敌人”（垃圾邮件发送者）持续创新话术，需系统自主适应

#### 案例4：股票价格预测（回归任务）
- 任务：预测未来某一日期的股票价格（如S&P/TSX综合指数）
- 特点：输出为连续值，属于典型回归问题

#### 案例5：计算生物学（回归任务）
- 任务：基于蛋白质序列，预测其3D结构（如蛋白质1IMT的结构预测）

#### 案例6：推荐系统（关联挖掘任务）
- 核心逻辑：基于用户行为关联推荐（如“购买A书籍的用户也购买B书籍”）
- 示例：用户购买Hastie相关书籍后，推荐《Pattern Recognition and Machine Learning》

#### 案例7：文档结构可视化（降维任务）
- 方法：潜在语义分析（LSA，PCA的一种形式）、自编码器（Autoencoder）
- 流程：将文档转化为词频向量→映射到2D坐标→以彩色点展示（颜色代表人工标注类别）
- 评估标准：无需使用类别标签建模，通过2D空间中同类文档的聚集程度判断算法效果
- 应用示例：将文档聚类为“银行市场”“货币/经济”“欧盟”“能源市场”等主题

## 九、学习任务类型细分（第22-26页）
### 1. 监督学习（Supervised Learning）
- 核心定义：从带标签的训练数据中推断函数（infer a function from labeled training data）
- 示例：“苹果”的文字与图像标注对应学习
- 核心任务：分类（离散输出）、回归（连续输出）

### 2. 无监督学习（Unsupervised Learning）
- 核心定义：从无标签的训练数据中寻找隐藏结构（find hidden structure in unlabeled training data）
- 核心任务：聚类（Clustering，数据自主分组）、降维（Dimensionality reduction，简化数据维度）

### 3. 强化学习（Reinforcement Learning）
- 核心定义：在动态环境中学习行动策略，通过获取奖励优化行为（learn a policy of taking actions in a dynamic environment and acquire rewards）
- 核心要素：环境（environment）→ 智能体（agent）→ 行动（action）→ 新状态（new state）→ 奖励（reward）的循环

### 4. 任务类型对应表
| 学习类型       | 核心任务               |
|----------------|------------------------|
| 监督学习       | 分类（classification）、回归（regression） |
| 无监督学习     | 聚类（clustering）、降维（dimensionality reduction） |

## 十、机器学习发展历史（第27-31页）
按时间线梳理关键突破与技术演进：

### 1. 1950s
- 核心成果：Samuel的跳棋程序（Samuel’s checker player）、Selfridge的Pandemonium系统

### 2. 1960s
- 核心成果：神经网络（感知机Perceptron）、模式识别（Pattern recognition）、极限学习理论（Learning in the limit theory）
- 关键事件：明斯基和佩珀特证明感知机的局限性（Minsky and Papert prove limitations of Perceptron）

### 3. 1970s
- 核心成果：符号概念归纳（Symbolic concept induction）、Winston的拱门学习器（Winston’s arch learner）、专家系统（Expert systems）、Quinlan的ID3算法、AM数学发现系统（Mathematical discovery with AM）
- 关键问题：专家系统面临知识获取瓶颈（knowledge acquisition bottleneck）

### 4. 1980s
- 核心成果：高级决策树与规则学习、基于解释的学习（EBL）、学习与规划/问题求解融合、类比学习（Analogy）、认知架构（Cognitive architectures）、神经网络复兴（Resurgence of neural networks）、瓦里安的PAC学习理论（Probably approximately correct Learning Theory）
- 关键问题：效用问题（Utility problem）

### 5. 1990s
- 核心成果：数据挖掘（Data mining）、自适应软件智能体与网页应用、文本学习（Text learning）、强化学习（RL）、归纳逻辑编程（ILP）、集成学习（Ensembles：Bagging、Boosting、Stacking）、贝叶斯网络学习（Bayes Net learning）、支持向量机（Support vector machines）、核方法（Kernel methods）

### 6. 2000s
- 核心成果：图模型（贝叶斯网络、马尔可夫随机场）、变分推断（Variational inference）、统计关系学习（Statistical relational learning）、迁移学习（Transfer learning）、序列标注（Sequence labeling）、集体分类与结构化输出（Collective classification and structured outputs）
- 应用领域：编译器、调试、图形学、安全（入侵/病毒/蠕虫检测）、邮件管理、个性化助手、机器人与视觉学习

### 7. 2010s
- 核心成果：深度学习（DNN、CNN、RNN）、ImageNet挑战赛、大数据学习、GPU/HPC支撑的学习、生成对抗网络（GANs）、强化学习突破（AlphaGo）、多任务与终身学习（Multi-task & lifelong learning）、深度强化学习、自然语言处理（Transformer、BERT、GPT）

### 8. 2020s+（前沿方向）
- 核心方向：自监督学习（Self-supervised Learning）、多模态学习（CLIP）、Transformer模型（GPT-3、BERT）、扩散模型（DALL-E 2、Stable Diffusion）、大语言模型（LLMs）、联邦学习（Federated Learning）、量子机器学习（Quantum Machine Learning）、人工智能伦理与公平性（Ethics and Fairness in AI）、可解释人工智能（XAI：LIME、SHAP）

## 十一、机器学习的一般过程（第32页）
### 1. 核心流程
原始数据（Raw Data）→ 数据格式化（Formalization）→ 训练数据（Training Data）/ 测试数据（Test Data）→ 模型训练（Model）→ 模型评估（Evaluation）

### 2. 核心假设
训练数据与测试数据遵循相同的潜在模式（there exist the same patterns across training and test data）——这是模型泛化能力的前提。

## 十二、图像分类问题举例（第33-36页）
### 1. 完整流程拆解
#### 训练阶段
训练图像（Training Images）+ 训练标签（Training Labels）→ 提取特征（Features）→ 学习模型（Learned model）

#### 测试阶段
测试图像（Test Image）→ 提取特征（Features）→ 输入学习模型（Learned model）→ 输出预测结果（Prediction）

### 2. 特征提取类型
- 原始像素（Raw pixels）
- 直方图（Histogram）
- GIST描述符（GIST descriptors）
- 边缘方向（Edge Orientation）

### 3. 数学表达
- 预测函数：\(f(s) = "apple"\)、\(f(S)= "tomato"\)、\(f(w)= "COW"\)（输入特征向量，输出类别标签）
- 通用公式：\(y=f(x)\)（\(x\)为输入实例，\(y\)为输出标签）
- 训练目标：给定带标签训练集\(\{(x^{(i)}, y^{(i)}), ..., (x^{(N)}, y^{(N)})\}\)，通过最小化训练集预测误差，估计预测函数\(f\)
- 测试目标：将\(f\)应用于未见过的测试样本\(x\)，输出预测值\(y=f(x)\)

## 十三、监督学习详解（第37-39页）
### 1. 输入数据定义
带标签的训练数据集：\(D=\left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1,2,...,N}\)
- \(x^{(i)}\)：第\(i\)个训练样本的输入数据（特征，Input data/features）
- \(y^{(i)}\)：第\(i\)个训练样本的输出数据（标签，Output data/label）

### 2. 学习目标
学习从数据到标签的映射函数：\(y^{(i)} \approx f_{\theta }\left(x^{(i)}\right)\)
- 假设空间（Hypothesis Space）：函数集\(\{f_{\theta}(x^{(i)})\}\)（所有可能的映射函数集合）
- 学习本质：更新参数\(\theta\)，使预测结果尽可能接近对应标签（make the prediction close to the corresponding label）

### 3. 核心问题
1. 学习目标是什么？（What is the Learning Objective?）
2. 如何更新参数？（How to update the parameters?）

## 十四、核心问题1：学习目标（第40-43页）
### 1. 核心目标
使预测结果接近对应标签（Make the prediction close to the corresponding label）

### 2. 风险最小化框架
- 期望风险（Expected Risk/Error Minimization）：理论最优目标，但无法直接计算（需遍历所有可能数据）
- 经验风险最小化（Empirical Risk Minimization, ERM）：实际可操作目标，公式为：
  \[min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)\]
  - \(L(y^{(i)}, f_{\theta}(x^{(i)}))\)：损失函数，衡量单个样本的标签与预测值之间的误差
  - 损失函数定义：依赖数据类型和具体任务

### 3. 常见损失函数
#### （1）0-1损失函数（0-1 Loss Function）
\[L\left(y^{(i)}, f\left(x^{(i)}\right)\right)= \begin{cases}1, & if y^{(i)} \neq f\left(x^{(i)}\right) \\ 0, & if y^{(i)}=f\left(x^{(i)}\right)\end{cases}\]
- 特点：预测错误则损失为1，正确则为0，适用于分类任务。

#### （2）均方误差（Mean Squared Error, MSE）
\[L\left(y^{(i)}, f\left(x^{(i)}\right)\right)=\left(y^{(i)}-f\left(x^{(i)}\right)\right)^{2}\]
- 变体（常用）：\(L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)=\frac{1}{2}\left(y^{(i)}-f_{\theta}\left(x^{(i)}\right)\right)^{2}\)
- 特点：对大误差惩罚更重，接受小误差（如观测噪声），利于模型泛化，适用于回归任务。

#### （3）绝对损失函数（Absolute Loss Function）
\[L\left(y^{(i)}, f\left(x^{(i)}\right)\right)=\left|y^{(i)}-f\left(x^{(i)}\right)\right|\]
- 特点：对误差的惩罚线性分布，受异常值影响小于MSE。

#### （4）对数损失函数（交叉熵损失，Logarithmic Loss Function/Cross-Entropy Loss Function）
\[L\left(y^{(i)}, p^{(i)}\right)=-\left[y^{(i)} log \left(p^{(i)}\right)+\left(1-y^{(i)}\right) log \left(1-p^{(i)}\right)\right]\]
- 适用场景：二分类任务（\(y^{(i)} \in \{0,1\}\)）
- 变量说明：\(p^{(i)}=f(x^{(i)})\)，表示第\(i\)个样本属于正类（通常为类别1）的预测概率。

## 十五、核心问题2：如何更新参数（第44页）
### 1. 核心方法：梯度下降法（Gradient Learning Methods）
- 核心逻辑：计算损失函数对参数的偏导数\(\frac{\partial J(\theta)}{\partial \theta_{i}}\)（\(i=0,1,...,n\)），沿梯度负方向更新参数，最小化损失。
- 更新公式：\[\theta_{new } \leftarrow \theta_{old }-\eta \frac{\partial \mathcal{L}(\theta)}{\partial \theta}\]
  - \(\eta\)：学习率（Learning Rate），控制参数更新步长。

### 2. 其他优化算法
- 牛顿法（Newton method）
- 拟牛顿法（Quasi-Newton method）
- 共轭梯度法（Conjugate gradient method）

## 十六、超参数（第45-46页）
### 1. 定义
机器学习模型中，训练前预先设定、不通过数据学习的参数（settings set before training, not learned from data），对模型性能有显著影响。

### 2. 关键超参数类型
- 学习率（Learning Rate）：控制模型学习速度
- 正则化系数（Regularization Coefficient）：通过惩罚复杂模型防止过拟合
- 批次大小（Batch Size）：单次参数更新使用的训练样本数
- 迭代次数（Number of Epochs）：整个数据集遍历的次数
- 模型架构（Model Architecture）：神经网络的层数、神经元数等
- 优化器（Optimizer）：用于最小化损失函数的算法

### 3. 超参数优化方法
- 网格搜索（Grid Search）：穷举所有可能的超参数组合
- 随机搜索（Random Search）：随机采样超参数组合
- 贝叶斯优化（Bayesian Optimization）：构建代理模型指导超参数选择
- 进化算法（Evolutionary Algorithms）：模拟自然选择优化超参数
- 基于梯度的优化（Gradient-based Optimization）：利用梯度信息优化可微超参数
- 优化工具库：Optuna、Hyperopt、Ray Tune

## 十七、模型选择（第47-49页）
### 1. 欠拟合与过拟合
#### （1）欠拟合（Underfitting）
- 定义：模型无法捕捉数据的潜在趋势（cannot capture the underlying trend of the data）
- 示例：线性模型拟合非线性数据

#### （2）过拟合（Overfitting）
- 定义：模型拟合了数据中的随机误差或噪声，而非潜在关系（describes random error or noise instead of the underlying relationship）
- 示例：高次多项式模型拟合含噪声的数据

#### （3）直观表现（训练/测试误差曲线）
- 欠拟合：训练误差和测试误差均较高，且两者差距小
- 过拟合：训练误差低，测试误差高，且两者差距大

### 2. 偏差-复杂度权衡（Bias-Complexity Trade-off）
- 核心规律：模型复杂度增加时，训练损失持续下降，但测试损失先降后升
- 最优选择：测试损失最低的模型（兼顾偏差和方差）

## 十八、奥卡姆剃刀原则（第50页）
### 1. 核心思想
在多个竞争假设中，应选择假设最少的那个（Among competing hypotheses, the one with the fewest assumptions should be selected）。

### 2. 数学表达（结构风险最小化，Structural Risk Minimization, SRM）
\[min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)+\lambda \Omega(\theta)\]
- 组成部分：
  - 原始损失（Original loss）：经验风险项，衡量模型拟合数据的能力
  - 假设惩罚项（Penalty on assumptions）：\(\lambda \Omega(\theta)\)，惩罚复杂假设（\(\lambda\)为正则化系数，\(\Omega(\theta)\)为模型复杂度度量）
- 目的：平衡模型拟合能力与复杂度，避免过拟合。

## 十九、正则化方法（第51页）
正则化是实现结构风险最小化的核心手段，通过惩罚模型复杂度防止过拟合：

### 1. L2范数（岭回归，Ridge）
\[min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)+\lambda\|\theta\|^{2}\]
- 特点：使参数值整体缩小，避免单个参数过大，提高模型泛化性。

### 2. L1范数（LASSO）
\[min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)+\lambda|\theta|\]
- 特点：可使部分参数变为0，实现特征选择，简化模型。

### 3. 弹性网（Elastic Net）
\[min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)+\lambda|\theta|+(1-\lambda)\|\theta\|^{2}\]
- 特点：结合L1和L2正则化的优势，既实现特征选择，又避免LASSO在样本量小时的不稳定性。

## 二十、损失函数、代价函数、目标函数的区别（第52页）
### 1. 损失函数（Loss Function）
- 定义：衡量单个样本的预测误差（误差的瞬时值）
- 公式：\(L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)\)

### 2. 代价函数（Cost Function）
- 定义：衡量所有训练样本的平均误差（误差的平均值）
- 公式：\(\frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)\)（即经验风险）

### 3. 目标函数（Objective Function）
- 定义：训练过程中需要最小化或最大化的函数，是模型优化的最终目标
- 构成：代价函数 + 正则化项（如L1、L2）
- 示例：
  \[\frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)+\lambda\|\theta\|^{2}\]
  \[\frac{1}{N} \sum_{i=1}^{N} L\left(y^{(i)}, f_{\theta}\left(x^{(i)}\right)\right)+\lambda|\theta|\]

## 二十一、模型选择方法：留出法（第53页）
### 1. 核心目的
将数据拆分，用于训练模型和选择最优模型（避免用测试集选择模型导致过拟合）。

### 2. 数据拆分比例
原始数据集 → 训练集（60%）、验证集（20%）、测试集（20%）
- 训练集：用于模型训练
- 验证集：用于模型选择（如超参数优化、正则化系数调整）
- 测试集：用于最终模型性能评估（独立于训练和选择过程）

## 二十二、模型选择方法：交叉验证（第54页）
### 1. k折交叉验证（k-fold cross validation）
#### 流程
1. 将训练集分为k个不相交的子集\(\{S_1, S_2, ..., S_k\}\)
2. 对于每个\(i=1..k\)：
   - 用\(S_{-i} = S - S_i\)（k-1个子集）训练模型，得到\(h_i = L(S_{-i})\)
   - 用\(S_i\)评估模型，计算误差\(err_{S_i}(h_i) = \frac{1}{|S_i|} \sum_{<x,y> \in S_i} I(h_i(x) \neq y)\)（\(I\)为指示函数）
3. 返回平均误差：\(\frac{1}{k} \sum_{i=1}^k err_{S_i}(h_i)\)

### 2. 留一交叉验证（leave-one-out cross validation）
- 特殊情况：k等于训练集样本数（\(K=|S|\)）
- 特点：每个子集仅含1个样本，评估结果稳定但计算量较大。

## 二十三、偏差和方差（第55-60页）
### 1. 真实值与模型误差的关系
- 真实值定义：\(y=f(x)+\epsilon\)（\(f(x)\)为真实函数，\(\epsilon\)为噪声，服从\(\epsilon ~ N(0, \sigma^2)\)）
- 模型均方误差（MSE）：\(E\left[(\hat{f}(x)-y)^{2}\right]\)（\(\hat{f}(x)\)为模型预测函数）

### 2. 均方误差分解
\[
\begin{align*}
E\left[(\hat{f}(x)-y)^{2}\right] &= E\left[(\hat{f}(x)-f(x)-\epsilon)^2\right] \\
&= E\left[(\hat{f}(x)-f(x))^2\right] + E\left[\epsilon^2\right] \\
&= E\left[(\hat{f}(x)-E[\hat{f}(x)])^2\right] + E\left[(E[\hat{f}(x)]-f(x))^2\right] + \sigma^2 \\
&= Variance + Bias^2 + \sigma^2
\end{align*}
\]
- 方差（Variance）：模型预测结果的波动程度（受数据噪声、模型复杂度影响）
- 偏差（Bias）：模型预测值与真实值的系统偏差（受模型复杂度影响）
- 噪声（\(\sigma^2\)）：数据本身的不可避免误差

### 3. 偏差和方差的直观表现
#### （1）高偏差（Low Variance, High Bias）
- 学习曲线特征：训练误差高，且训练误差与测试误差差距小
- 对应问题：欠拟合（模型复杂度不足）

#### （2）高方差（High Variance, Low Bias）
- 学习曲线特征：训练误差低，测试误差高，且两者差距大；测试误差随训练集增大持续下降
- 对应问题：过拟合（模型复杂度过高，或训练数据不足）

## 二十四、应对欠拟合和过拟合的方法（第61-62页）
### 1. 应对欠拟合（Fixes to underfitting）
- 增加模型复杂度（Increase model complexity）
- 扩展特征集（Try a larger set of features）
- 减少正则化强度（Reduce regularization）

### 2. 应对过拟合（Fixes to overfitting）
- 降低模型复杂度（Reduce model complexity）
- 增加训练数据（Try getting more training examples）
- 增强正则化（Increase regularization）
- 减少特征集（Try a smaller set of features）
- 集成学习（Ensemble learning）：结合多个模型降低泛化误差

## 二十五、模型评价标准（第63-67页）
评价标准需根据任务类型选择，核心分为回归、分类、多分类三类：

### 1. 回归任务评价标准（Regression Evaluation Metrics）
#### （1）均方误差（MSE）
\[MSE = \frac{1}{N} \sum_{i=1}^{N}\left(y^{(i)}-f\left(x^{(i)}\right)\right)^{2}\]
- 特点：惩罚大误差，单位为目标变量单位的平方。

#### （2）平均绝对误差（MAE）
\[MAE = \frac{1}{N} \sum_{i=1}^{N}\left|y^{(i)}-f\left(x^{(i)}\right)\right|\]
- 特点：对异常值鲁棒，单位与目标变量一致。

#### （3）均方根误差（RMSE）
\[RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(y^{(i)}-f\left(x^{(i)}\right)\right)^{2}}\]
- 特点：单位与目标变量一致，更直观反映误差大小。

#### （4）决定系数（R²，R-Squared）
\[R^2 = 1 - \frac{\sum_{i=1}^{N}\left(y^{(i)}-f\left(x^{(i)}\right)\right)^{2}}{\sum_{i=1}^{N}\left(y^{(i)}-\overline{y}\right)^{2}}\]
- 特点：衡量模型解释数据变异的能力，取值范围[0,1]，越接近1说明模型拟合效果越好。

### 2. 分类任务评价标准（Classification Evaluation Metrics）
#### （1）混淆矩阵（Confusion Matrix）
| 真实类别\预测类别 | 1（正类） | 0（负类） |
|-------------------|-----------|-----------|
| 1（正类）         | 真阳性（TP） | 假阴性（FN） |
| 0（负类）         | 假阳性（FP） | 真阴性（TN） |

#### （2）核心指标公式
- 准确率（Accuracy）：整体预测正确的比例
  \[Accuracy = \frac{TP + TN}{TP + TN + FP + FN}\]
- 精确率（Precision）：预测为正类的样本中，实际为正类的比例（关注“预测准不准”）
  \[Precision = \frac{TP}{TP + FP}\]
- 召回率（Recall）：实际为正类的样本中，被预测为正类的比例（关注“漏没漏”）
  \[Recall = \frac{TP}{TP + FN}\]
- F1分数：精确率和召回率的调和平均（平衡两者）
  \[F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}\]

#### （3）ROC曲线与AUC
- 真阳性率（TPR）：\(TPR = \frac{TP}{TP + FN}\)（召回率的另一种表述）
- 假阳性率（FPR）：\(FPR = \frac{FP}{TN + FP}\)
- ROC曲线：以FPR为横轴、TPR为纵轴绘制的曲线
- AUC（Area Under ROC Curve）：ROC曲线下的面积，衡量模型区分正负类的能力
  - 取值范围：[0.5, 1]
  - 完美预测：AUC=1
  - 随机预测：AUC=0.5

### 3. 多分类任务评价标准
- 核心逻辑：采用“一对多”策略（One-vs-Rest），将每个类别单独视为正类，其余类别视为负类，逐一计算二分类评价指标（如准确率、F1分数），最终取平均值。

## 二十六、监督学习回顾与假设空间（第68-69页）
### 1. 监督学习核心回顾
- 输入：带标签数据集\(D=\left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1,2,...,N}\)
- 目标：学习函数\(y^{(i)} \approx f_{\theta }\left(x^{(i)}\right)\)，更新参数\(\theta\)使预测接近标签
- 假设空间：函数集\(\{f_{\theta}(x^{(i)})\}\)（所有可能的映射函数）

### 2. 机器学习的“艺术”
监督学习可视为“探索假设空间”的过程，核心在于三点：
1. 确定输入与输出的表征方式（How to represent the inputs and outputs）
2. 选择假设空间：足够强大以表征输入输出关系，同时足够简单以方便搜索（powerful but simple enough）
3. 不同学习方法的差异：假设空间（表征语言）和搜索技术不同

## 二十七、函数表征方式（第70页）
机器学习中常见的函数表征（假设空间的具体形式）：

### 1. 数值函数（Numerical functions）
- 代表方法：线性回归（Linear regression）、神经网络（Neural networks）、支持向量机（Support vector machines）

### 2. 符号函数（Symbolic functions）
- 代表方法：决策树（Decision trees）、命题逻辑规则（Rules in propositional logic）、一阶谓词逻辑规则（Rules in first-order predicate logic）

### 3. 基于实例的函数（Instance-based functions）
- 代表方法：k近邻（Nearest-neighbor）、案例推理（Case-based）

### 4. 概率图模型（Probabilistic Graphical Models）
- 代表方法：朴素贝叶斯（Naïve Bayes）、贝叶斯网络（Bayesian networks）、隐马尔可夫模型（HMMs）、概率上下文无关文法（PCFGs）、马尔可夫网络（Markov networks）

## 二十八、搜索算法（第71页）
用于在假设空间中寻找最优函数的核心算法：

### 1. 梯度类算法（Gradient-based）
- 代表：梯度下降（Gradient descent）、感知机（Perceptron）、反向传播（Backpropagation）

### 2. 动态规划（Dynamic Programming）
- 应用：隐马尔可夫模型学习（HMM Learning）、概率上下文无关文法学习（PCFG Learning）

### 3. 分治算法（Divide and Conquer）
- 应用：决策树归纳（Decision tree induction）、规则学习（Rule learning）

### 4. 进化计算（Evolutionary Computation）
- 代表：遗传算法（Genetic Algorithms, GAs）、遗传编程（Genetic Programming, GP）、神经进化（Neuro-evolution）

## 二十九、课程安排（第72页）
### 1. 课程内容
1. 线性模型（回归）（Linear models (regression)）
2. 线性模型（分类）（Linear models (classification)）
3. 决策树（Decision tree）
4. 人工神经网络（Artificial neural network）
5. 深度学习（Deep learning）
6. 支持向量机（SVM）
7. 贝叶斯学习（Bayesian Learning）
8. 集成学习（Ensemble learning）
9. 聚类（Clustering）
10. 降维与特征选择（Dimension reduction & feature selection）
11. 前沿介绍（Frontier Introduction）

### 2. 实验任务（lab assignments）
- 分组方式：3人1组
- 核心任务：回归（regression）、分类（classification）、聚类（clustering）

## 三十、课程考核（第73页）
### 1. 考核比例
- 出勤与课堂测验（Class attendance & Quize）：15%
  - 要求：课堂测验需在每节课后提交给教师（Class quizzes MUST be handed to the teacher after each class）
- 项目（Project）：45%
  - 次数：3次
  - 提交要求：
    - 提交平台：canvas
    - 邮件主题：Student ID_Name_MLProject_X（X替换为1、2、3）
    - 文件名：Student ID_Name_MLProject_X.zip（X替换为1、2、3）
  - 豁免条件：学术论文成果优异（Excellent academic paper achievements）
- 期末考试（Final Exam）：40%

## 三十一、参考书目（第74页）
### 1. 中文教材
- 周志华. 《机器学习》. 清华大学出版社, 2016（“西瓜书”，作者为南京大学计算机科学与技术系主任、人工智能学院院长）
- 李航. 《统计学习方法》. 清华大学出版社, 2019（作者为字节跳动人工智能实验室总监，北京大学、南京大学客座教授）

### 2. 英文教材/资源
- Tom M. Mitchell. Machine Learning[M]. New York: McGraw-Hill Companies, Inc, 1997.
- Andrew Ng. Machine Learning[EB/OL]. Stanford University, 2014. https://www.coursera.org/course/ml（作者为斯坦福大学副教授，前百度大脑负责人、首席科学家）
- Christopher M. Bishop. Pattern Recognition and Machine Learning（简称PRML，中文译本《模式识别与机器学习》，机器学习领域经典教材）

## 三十二、预备知识（第75页）
学习本课程需具备的数学基础：
- 线性代数（Linear Algebra）
- 微积分（Calculus）
- 数学优化（Mathematical Optimization）
- 概率论（Probability Theory）
- 信息论（Information Theory）
- 参考资料：《数学基础》（https://nndl.github.io/）

## 三十三、学习资源（第76-78页）
### 1. 数据集资源（Datasets）
#### （1）通用性分析数据集
- UCI Repository：http://www.ics.uci.edu/~mlearn/MLRepository.html
- UCI KDD Archive：http://kdd.ics.uci.edu/summary.data.application.html
- Statlib：http://lib.stat.cmu.edu/
- Delve：http://www.cs.utoronto.ca/~delve/

#### （2）探索性分析数据集
- Kaggle数据集：https://www.kaggle.com/datasets

#### （3）深度学习数据集
- Deeplearning.net：深度学习算法基准测试数据集
- DeepLearning4J.org：深度学习研究高质量数据集
- https://paperswithcode.com/datasets

### 2. 期刊资源（Journals）
- Journal of Machine Learning Research（www.jmlr.org）
- Machine Learning
- Neural Computation
- Neural Networks
- IEEE Transactions on Neural Networks
- IEEE Transactions on Pattern Analysis and Machine Intelligence
- Annals of Statistics
- Journal of the American Statistical Association

### 3. 会议资源（Conferences）
- 国际会议：International Conference on Machine Learning（ICML）、Neural Information Processing Systems（NeurIPS，原NIPS）、Computational Learning Theory（COLT）、European Conference on Machine Learning（ECML）、Asian Conference on Machine Learning（ACML）、IEEE Conference on Computer Vision and Pattern Recognition（CVPR）、AAAI Conference on Artificial Intelligence（AAAI）、International Joint Conference on Artificial Intelligence（IJCAI）
- 国内会议：中国机器学习大会（CCML）、机器学习及其应用（MLA）

## 三十四、思考问题（第79-80页）
试分析什么因素会导致模型出现图中所示的高偏差和高方差情况（结合前文偏差-方差分解及模型复杂度分析）：
- 高偏差因素：模型复杂度不足（如用线性模型拟合非线性数据）、特征维度不够、正则化强度过大
- 高方差因素：模型复杂度过高（如高次多项式模型）、训练数据不足或含噪声过多、超参数设置不当（如学习率过大）、无正则化或正则化强度不足
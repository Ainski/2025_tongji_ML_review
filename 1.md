# 同济大学《机器学习-第2章：线性回归》课件逐页详解（按文档顺序）
## 一、开篇与学习任务类型（第1-3页）
### 1. 课件基础信息
- 授课院校：同济大学计算机科学与技术学院
- 授课教师：李洁、倪张凯（邮箱：nijanice@163.com、zkni@tongji.edu.cn；配套网址：https://eezkni.github.io/）
- 核心主题：第2章 线性回归（日期：9/22/2025）

### 2. 三大学习任务类型
- **监督学习（Supervised Learning）**：从带标签的训练数据中推断函数，核心是“给定每个样本的正确答案”，包含两类核心任务——分类（classification/categorization）和回归（regression）。
- **无监督学习（Unsupervised Learning）**：挖掘无标签训练数据的隐藏结构，核心是“无正确答案，找数据内在规律”，包含聚类（clustering）和降维（dimensionality reduction）。
- **强化学习（Reinforcement Learning）**：在动态环境中学习行动策略，通过执行动作获取奖励，核心是“试错学习，优化长期奖励”。

### 3. 学习任务类型对比表
| 监督学习核心任务 | 无监督学习核心任务 |
|------------------|--------------------|
| 分类（categorization） | 聚类（clustering） |
| 回归（regression）     | 降维（dimensionality reduction） |

## 二、机器学习一般过程（第4页）
### 1. 核心流程
原始数据（Raw Data）→ 形式化处理（Formalization）→ 拆分训练数据（Training Data）与测试数据（Test Data）→ 构建模型（Model）→ 评估（Evaluation）
### 2. 基础假设
训练数据与测试数据遵循相同的模式（patterns），这是机器学习模型能泛化到新数据的前提。

## 三、线性模型基础（第6-15页）
### 1. 线性模型的核心特点
易理解（easily understood）、易实现（implemented）、高效且可扩展（efficient and scalable），主要包括线性回归（Linear Regression）和线性分类（Linear Classification）两类。

### 2. 线性模型通用公式
\[f_{\theta}(x)=\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{n}x_{n}+\theta_{0}\]
- 符号说明：
  - \(x=(x_1,x_2,\dots,x_n)^T\)：样本的特征向量（Feature vector），\(x_1,x_2,\dots,x_n\) 是样本的n个特征/变量；
  - \(\theta_0\)：偏置项（常数项）；
  - \(\theta_1,\theta_2,\dots,\theta_n\)：特征对应的权重参数（需通过学习优化）。

### 3. 实例：西瓜打分模型（源自周志华《机器学习》“西瓜书”）
\[f_{好瓜}(x)=0.2 \cdot x_{色泽}+0.5 \cdot x_{根蒂}+0.3 \cdot x_{敲声}+1\]
通过色泽、根蒂、敲声三个特征的线性组合，对“是否为好瓜”进行量化打分，直观体现线性模型的应用逻辑。

## 四、线性回归的维度区分（第16-19页）
### 1. 单变量线性回归（One-dimensional regression）
- 特征数量：仅1个输入特征（variable）；
- 模型公式：\[f_{\theta}(x)=\theta_{1}x+\theta_{0}\]
- 核心场景：通过单一特征预测连续值（如“房屋面积→房价”）。

### 2. 双变量线性回归（Two-dimensional linear regression）
- 特征数量：2个输入特征；
- 模型公式：\[f_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}\]
- 核心场景：通过两个相关特征共同预测（如“房屋面积+卧室数→房价”）。

## 五、单变量线性回归详解（第20-37页）
### 1. 应用场景与训练数据
以波特兰房价预测为例，给出训练集（部分数据如下表），明确核心符号定义：
| 房屋面积（平方英尺，x） | 房价（千美元，y） |
|--------------------------|--------------------|
| 2104                     | 460                |
| 1416                     | 232                |
| 1534                     | 315                |
| 852                      | 178                |
- 符号说明：N=训练样本数量；x=输入特征（自变量）；y=输出标签（目标变量，连续值）。

### 2. 假设函数与参数
- 假设函数（Hypothesis）：\[f_{\theta}(x)=\theta_{0}+\theta_{1}x\]（即单变量线性回归模型）；
- 参数（Parameters）：\(\theta_0\)（截距）、\(\theta_1\)（斜率），学习的核心是找到最优\(\theta_0、\theta_1\)，使预测值\(f_{\theta}(x)\)接近真实值y。

### 3. 代价函数（Cost Function）
#### （1）定义
衡量预测值与真实值差距的量化指标，单变量线性回归采用**均方误差变体**：
\[J(\theta_{0},\theta_{1})=\frac{1}{2N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2\]
- 分母“2”是为了后续求导简化计算，不影响最优参数的求解；
- \(x^{(i)}、y^{(i)}\)表示第i个训练样本的特征和标签。

#### （2）简化场景（固定\(\theta_0=0\)）
若假设函数简化为\[f_{\theta}(x)=\theta_1x\]，则代价函数简化为：
\[J(\theta_1)=\frac{1}{2N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2\]

#### （3）可视化
- 假设函数可视化：固定\(\theta_1\)时，f(x)是关于x的直线（x为横轴，y为纵轴）；
- 代价函数可视化：以\(\theta_1\)为横轴，J(\(\theta_1\))为纵轴，曲线呈“碗状”（凸函数），存在唯一最小值。

## 六、单变量线性回归的参数求解：梯度下降（第38-48页）
### 1. 梯度下降核心逻辑
- 目标：最小化代价函数\(J(\theta_0,\theta_1)\)；
- 原理：从初始参数值出发，沿代价函数梯度的**反方向**迭代更新参数，逐步逼近最小值（“下山”逻辑）；
- 关键要求：参数必须**同时更新**（避免用更新后的参数计算另一个参数）。

### 2. 梯度下降更新公式
\[
\begin{cases}
\theta_0:=\theta_0-\alpha \cdot \frac{1}{N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)}) \\
\theta_1:=\theta_1-\alpha \cdot \frac{1}{N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
\end{cases}
\]
- \(\alpha\)：学习率（Learning Rate），控制每一步更新的“步长”；
- 公式推导：对代价函数\(J(\theta_0,\theta_1)\)分别求\(\theta_0、\theta_1\)的偏导，梯度方向为偏导方向，反方向即为下降最快方向。

### 3. 可视化与更新原则
- 2D可视化：以\(\theta_0\)或\(\theta_1\)为横轴，J为纵轴，展示从初始点到最小值的迭代轨迹；
- 3D可视化：以\(\theta_0、\theta_1\)为横轴，J为纵轴，代价函数呈“碗状曲面”，梯度下降沿曲面下滑至最低点；
- 正确更新：先计算所有参数的临时更新值（temp0、temp1），再同时替换\(\theta_0、\theta_1\)；
- 错误更新：用更新后的\(\theta_0\)计算\(\theta_1\)（或反之），导致迭代方向偏离最优路径。

## 七、凸函数与搜索过程（第49-51页）
### 1. 凸函数特性
代价函数\(J(\theta_0,\theta_1)\)是**碗状凸函数（Bowled shape Convex Function）**，具有唯一最小值（全局最优解），无论初始参数如何选择，梯度下降最终都会收敛到该最小值（不会陷入局部最优）。

### 2. 搜索流程
1. 初始化参数\(\theta_0、\theta_1\)（通常设为0或随机小值）；
2. 用梯度下降公式迭代更新参数；
3. 直到代价函数\(J(\theta)\)不再明显下降（收敛），停止迭代。

## 八、梯度下降的三种变种（第52-54页）
### 1. 批量梯度下降（Batch Gradient Descent, BGD）
- 核心：每次参数更新使用**全部训练样本**计算梯度；
- 公式：与单变量梯度下降公式一致（求和范围为所有N个样本）；
- 特点：梯度估计准确，收敛稳定，但计算量大，适用于小数据集。

### 2. 随机梯度下降（Stochastic Gradient Descent, SGD）
- 核心：每次参数更新仅使用**1个训练样本**计算梯度；
- 公式：
\[
\begin{cases}
\theta_0:=\theta_0-\alpha \cdot (f_{\theta}(x^{(i)})-y^{(i)}) \\
\theta_1:=\theta_1-\alpha \cdot (f_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
\end{cases}
\]
- 特点：计算速度快，适合大规模/在线学习，但梯度波动大（不确定性），收敛路径震荡。

### 3. 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）
- 核心：结合BGD和SGD，每次使用**k个迷你批次（mini-batch）** 样本计算梯度；
- 公式：
\[
\begin{cases}
\theta_0:=\theta_0-\alpha \cdot \frac{1}{N_k}\sum_{i=1}^{N_k}(f_{\theta}(x^{(i)})-y^{(i)}) \\
\theta_1:=\theta_1-\alpha \cdot \frac{1}{N_k}\sum_{i=1}^{N_k}(f_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
\end{cases}
\]
- 特点：兼顾BGD的稳定性和SGD的收敛速度，是实际应用中最常用的变种。

## 九、学习率选择（第55-56页）
### 1. 学习率的影响
- 学习率过小（α太小）：梯度下降收敛极慢（需大量迭代步骤）；
- 学习率过大（α太大）：可能跳过代价函数最小值，导致震荡不收敛甚至发散。

### 2. 验证与调整方法
- 每迭代几次或几十次，打印代价函数\(J(\theta)\)；
- 若\(J(\theta)\)持续下降，说明学习率合适；若下降缓慢或上升，需调整α（常用范围：0.001、0.01、0.1）。

### 3. 可视化对比
- 损失曲线（Loss vs Iteration）：合适的α使J(θ)快速下降并趋于平稳；过小的α下降缓慢；过大的α可能上升；
- 参数轨迹（Parameter trajectory）：合适的α使参数快速逼近最优值，过大的α轨迹震荡。

## 十、多变量线性回归（第57-63页）
### 1. 应用场景与训练数据
当预测目标受多个特征影响（如房价受面积、卧室数、楼层、房龄共同影响），训练集如下（部分数据）：
| 房屋面积（平方英尺） | 卧室数 | 楼层数 | 房龄（年） | 房价（千美元） |
|----------------------|--------|--------|------------|----------------|
| 2104                 | 5      | 1      | 45         | 460            |
| 1416                 | 3      | 2      | 40         | 232            |
| 1534                 | 3      | 2      | 30         | 315            |
| 852                  | 2      | 1      | 36         | 178            |

### 2. 假设函数与代价函数
#### （1）假设函数
引入\(x_0=1\)（统一特征格式，方便矩阵运算），公式为：
\[f_{\theta}(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n\]
- n=特征数量，\(x_j^{(i)}\)表示第i个样本的第j个特征。

#### （2）代价函数
\[J(\theta_0,\theta_1,\dots,\theta_n)=\frac{1}{2N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2\]
与单变量逻辑一致，仅参数数量扩展到n+1个（\(\theta_0-\theta_n\)）。

### 3. 多变量梯度下降公式
扩展单变量逻辑，参数更新公式统一为：
\[\theta_j:=\theta_j-\alpha \cdot \frac{1}{N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} \quad (j=0,1,\dots,n)\]
- 要求：所有参数同时更新。

### 4. 多变量回归的核心问题
- 多重共线性（Multicollinearity）：多个特征高度相关（如“房屋总面积”和“卧室总面积”），导致模型参数不稳定；
- 特征尺度差异（Scaling Issues）：不同特征量级差异大（如面积0-2000平方英尺，卧室数1-5），减慢梯度下降收敛速度。

## 十一、多变量回归的解决方案（第64-70页）
### 1. 特征缩放（Feature Scaling）
#### （1）核心目标
使所有特征处于相似尺度，加速梯度下降收敛。

#### （2）两种常用方法
- 归一化（Normalization）：\[x'=\frac{x-mean(x)}{max(x)-min(x)}\]（将特征缩放到[0,1]或[-1,1]区间）；
- 标准化（Standardization）：\[x'=\frac{x-mean(x)}{std(x)}\]（使特征均值为0，标准差为1，更常用）；
- 实例：面积特征\(x_1=\frac{面积（平方英尺）}{2000}\)，卧室数特征\(x_2=\frac{卧室数}{5}\)，统一到[0,1]区间。

### 2. 自适应学习率优化器
针对固定学习率的不足，提出4种自适应优化器，解决收敛速度和稳定性问题：
- **Momentum**：累积历史梯度，平滑优化过程，加速一致方向的收敛（类似“惯性”）；
- **Adagrad**：按参数调整学习率（用过去梯度平方和的平方根归一化），适合稀疏数据（如部分特征极少出现）；
  - 公式：\[\theta^{(t+1)}:=\theta^{(t)}-\frac{\alpha}{\sqrt{\sum_{i=0}^{t}(g^{(i)})^2+\varepsilon}}g^{(t)}\]（\(g^{(t)}=\frac{\partial J(\theta^{(t)})}{\partial \theta}\)，\(\varepsilon\)避免分母为0）；
- **RMSProp**：用指数衰减平均的梯度平方调整学习率，避免学习率过度缩小，适合非平稳目标函数；
- **Adam**：结合Momentum（一阶矩）和RMSProp（二阶矩），收敛最快且最稳定，是深度学习中最常用的优化器。

#### （3）优化器轨迹对比
- Momentum：路径有惯性，可能先震荡再稳定；
- Adagrad：步长逐渐缩小，后期收敛变慢；
- RMSProp：学习率稳定，路径平稳；
- Adam：兼顾速度与稳定性，整体最优。

## 十二、正则化方法（第71-72页）
### 1. 核心目标
解决多变量回归的**过拟合**和**多重共线性**问题，通过在代价函数中添加参数惩罚项，限制参数绝对值大小。

### 2. 三种常用正则化
- **L2正则化（Ridge回归）**：惩罚参数的平方和，公式：
  \[min_{\theta}\frac{1}{2N}\sum_{i=1}^{N}L(y^{(i)},f_{\theta}(x^{(i)}))+\lambda\|\theta\|^2\]
  - 特点：参数仅缩小，不置零，保留所有特征，可解决\(X^TX\)不可逆问题；
- **L1正则化（LASSO回归）**：惩罚参数的绝对值，公式：
  \[min_{\theta}\frac{1}{2N}\sum_{i=1}^{N}L(y^{(i)},f_{\theta}(x^{(i)}))+\lambda|\theta|\]
  - 特点：使部分参数收缩至0，实现特征选择（剔除冗余特征）；
- **弹性网（Elastic Net）**：结合L1和L2正则化，公式：
  \[min_{\theta}\frac{1}{2N}\sum_{i=1}^{N}L(y^{(i)},f_{\theta}(x^{(i)}))+\lambda|\theta|+(1-\lambda)\|\theta\|^2\]
  - 特点：兼顾特征选择和参数稳定性，避免LASSO在特征多时有偏。

## 十三、回归方法扩展（第73-78页）
### 1. 线性回归扩展（含正则化）
| 回归方法       | 目标函数公式                                                                 |
|----------------|------------------------------------------------------------------------------|
| 普通线性回归   | \(min_{\theta}\frac{1}{2N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2\)     |
| Ridge回归      | \(min_{\theta}\frac{1}{2N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\|\theta\|^2\) |
| LASSO回归      | \(min_{\theta}\frac{1}{2N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2+\lambda|\theta|\) |
| 弹性网回归     | \(min_{\theta}\frac{1}{2N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2+\lambda|\theta|+(1-\lambda)\|\theta\|^2\) |

### 2. 非线性回归方法
当数据呈现非线性关系时，线性回归无法拟合，需使用非线性方法：
- 多项式回归（Polynomial Regression）：通过添加特征的高次项（如\(x^2、x^3\)）建模非线性，公式示例：
  \[f_{\theta}(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\cdots\]
  - 风险：高次多项式易过拟合（如50次多项式，训练误差极小但测试误差极大）；
- 其他非线性方法：决策树回归（Decision Tree Regression）、神经网络（Neural Networks）、支持向量回归（SVR）、梯度提升回归（Gradient Boosting Regression）。

## 十四、最小二乘法（第79-87页）
### 1. 核心定位
除梯度下降外，线性回归参数求解的另一种方法——直接通过数学推导得到全局最优解，无需迭代。

### 2. 矩阵形式推导
#### （1）代价函数矩阵化
\[J(\theta)=\frac{1}{N}\sum_{i=1}^{N}(f_{\theta}(x^{(i)})-y^{(i)})^2=\|X\theta-y\|^2=(X\theta-y)^T(X\theta-y)\]
- \(X\)：N×(n+1)特征矩阵（每行是一个样本，第一列全为1对应\(x_0\)）；
- \(\theta\)：(n+1)×1参数向量；
- \(y\)：N×1标签向量。

#### （2）求导与最优解
对\(J(\theta)\)求\(\theta\)的偏导，令梯度为0，解得：
\[\theta=(X^TX)^{-1}X^Ty\]
- 前提：\(X^TX\)可逆（非奇异矩阵）。

#### （3）Ridge回归的最小二乘解
当\(X^TX\)不可逆（如多重共线性），添加L2正则化后，解为：
\[\theta=(X^TX+\lambda I)^{-1}X^Ty\]
- \(I\)：单位矩阵，确保\(X^TX+\lambda I\)可逆。

### 3. 实例演示
以多变量房价数据为例，特征矩阵X为4×5（4个样本，5个特征：\(x_0-x_4\)），参数向量\(\theta\)为5×1（\(\theta_0-\theta_4\)），代入公式即可直接计算最优参数。

## 十五、梯度下降vs最小二乘法（第88-89页）
| 对比维度       | 梯度下降（Gradient Descent）                                                | 最小二乘法（Least Square Method）                                            |
|----------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------|
| 计算方式       | 迭代优化                                                                   | 直接矩阵运算（无迭代）                                                       |
| 核心优势       | 1. 适用于大规模/高维数据；2. 支持在线/增量学习；3. 可搭配自适应优化器；4. 内存需求低 | 1. 直接得全局最优解；2. 无需调超参数（学习率、批次大小等）；3. 计算逻辑简洁 |
| 核心劣势       | 1. 收敛速度可能较慢；2. 需调学习率；3. 依赖特征缩放；4. 不当学习率导致不收敛 | 1. 计算复杂度高（\(O(n^3)\)，n为特征数）；2. 内存需求大；3. \(X^TX\)不可逆时无法直接计算；4. 不适用于高维/大规模数据 |
| 适用场景       | 大数据集、高维特征（如n>1000）                                             | 小数据集、特征数少（如n<100）                                                |

## 十六、回归评价指标（第90页）
用于量化回归模型的预测效果，核心指标如下：
- **均方误差（MSE）**：\(\frac{1}{N}\sum_{i=1}^{N}(y^{(i)}-f(x^{(i)}))^2\)，惩罚大误差（平方放大）；
- **平均绝对误差（MAE）**：\(\frac{1}{N}\sum_{i=1}^{N}|y^{(i)}-f(x^{(i)})|\)，对异常值鲁棒（无平方放大）；
- **均方根误差（RMSE）**：\(\sqrt{\frac{1}{N}\sum_{i=1}^{N}(y^{(i)}-f(x^{(i)}))^2}\)，与原数据量级一致，直观易理解；
- **决定系数（R²/R-squared）**：\(R^2=1-\frac{\sum_{i=1}^{N}(y^{(i)}-f(x^{(i)}))^2}{\sum_{i=1}^{N}(y^{(i)}-\overline{y})^2}\)，衡量模型对数据的解释力（范围[0,1]，越接近1越好）；
  - \(\overline{y}\)：标签的平均值，分母是“基准模型（预测值=均值）”的误差。
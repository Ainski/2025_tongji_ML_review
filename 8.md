这份文档是同济大学《机器学习》第8章“贝叶斯学习”的课件，按页面顺序可分为**基础概率理论、贝叶斯核心方法、分类器应用、进阶模型与算法**四大模块，以下是逐模块的详细解读：


### 一、基础概率理论（第6-13页）
这部分是贝叶斯学习的数学基础，核心覆盖3个核心概念：

#### 1. 全概率公式（Total Probability Theorem）
- **核心条件**：事件集合\(B_1,B_2,...,B_n\)需满足「互斥性」（\(B_i \cap B_j = \emptyset\)）和「穷尽性」（\(\sum_{i=1}^n P(B_i) = 1\)），即所有事件无重叠且覆盖全部可能。
- **公式**：\(P(A) = P(B_1)P(A|B_1) + P(B_2)P(A|B_2) + ... + P(B_n)P(A|B_n)\)
- **意义**：将复杂事件\(A\)的概率，分解为多个互斥子事件\(B_i\)条件下的概率加权和，简化计算。

#### 2. 条件概率（Conditional Probability）
- **定义**：已知事件\(B\)发生时，事件\(A\)发生的概率。
- **公式**：\(p(A|B) = \frac{p(A,B)}{p(B)}\)
  - 分子\(p(A,B)\)：\(A\)和\(B\)的联合概率（同时发生的概率）；
  - 分母\(p(B)\)：\(B\)的边缘概率（也叫先验概率，不考虑\(A\)时\(B\)的概率）。

#### 3. 独立性（Independence）
- **定义**：若事件\(A\)和\(B\)独立，则一个事件的发生不影响另一个的概率。
- **数学表达**：\(p(A|B) = p(A)\)（或等价于\(p(A,B) = p(A)p(B)\)）；
- 反之，若\(p(A|B) \neq p(A)\)，则\(A\)和\(B\)存在依赖关系。


### 二、贝叶斯核心方法（第10-54页）
这部分是贝叶斯学习的核心，包括贝叶斯定理、最大后验假设（MAP）、贝叶斯最优分类器。

#### 1. 贝叶斯定理（Bayes’ Theorem）
- **核心用途**：通过「可观测变量\(D\)」估计「隐藏的因果变量\(h\)」的后验概率（即已知\(D\)后，\(h\)的真实状态概率）。
- **公式**：\(P(h|D) = \frac{P(D|h)P(h)}{P(D)}\)
  - 后验概率\(P(h|D)\)：需估计的核心（直接计算困难）；
  - 似然\(P(D|h)\)：给定假设\(h\)时，观测到数据\(D\)的概率（易建模，因\(h\)是\(D\)的因）；
  - 先验概率\(P(h)\)：未观测\(D\)时，假设\(h\)成立的初始概率（基于领域知识或经验）；
  - 证据\(P(D)\)：数据\(D\)的边缘概率（对所有\(h\)为常数，可忽略）。
- **推导过程**：由条件概率定义推导：
  1. \(p(A|B)p(B) = p(A,B)\)（联合概率分解）；
  2. \(p(B|A)p(A) = p(A,B)\)（同理）；
  3. 联立得\(p(A|B) = \frac{p(B|A)p(A)}{p(B)}\)。

#### 2. 最大后验假设（MAP）
- **定义**：在假设空间\(H\)中，找到「给定数据\(D\)后概率最大的假设」，记为\(h_{MAP}\)。
- **公式推导**：
  \[
  h_{MAP} = \arg\max_{h \in H} P(h|D) = \arg\max_{h \in H} \frac{P(D|h)P(h)}{P(D)} = \arg\max_{h \in H} P(D|h)P(h)
  \]
  （因\(P(D)\)对所有\(h\)是常数，可省略）。
- **关键提醒**：\(h_{MAP}\)≠最可能的分类！  
  示例：假设空间\(H = \{h_1,h_2,h_3\}\)，观测数据\(D\)后：
  - \(P(h_1|D)=0.4\)，\(h_1(x)=+\)；
  - \(P(h_2|D)=0.3\)，\(h_2(x)=-\)；
  - \(P(h_3|D)=0.3\)，\(h_3(x)=-\)；
  - \(h_{MAP} = h_1(x)=+\)（单个假设概率最大）；
  - 但最可能的分类是\(-\)（加权和：\(\sum P(-|h_i)P(h_i|D) = 0.6\)，\(\sum P(+|h_i)P(h_i|D) = 0.4\)）。

#### 3. 贝叶斯最优分类器（Bayes Optimal Classifier）
- **定义**：不依赖单个最优假设，而是「结合所有假设的预测，按后验概率加权」，找到最可能的分类结果。
- **公式**：
  \[
  \arg\max_{v_j \in V} \sum_{h_i \in H} P(v_j|h_i)P(h_i|D)
  \]
  - \(V\)：所有可能的分类结果集合（如\(\{+,-\}\)）；
  - \(P(v_j|h_i)\)：假设\(h_i\)对分类\(v_j\)的预测（若\(h_i(x)=v_j\)则为1，否则为0）。
- **特性**：
  - 给出分类误差的「理论下界」（最优性能）；
  - 计算成本极高（需遍历所有假设），实际应用受限。


### 三、分类器应用：朴素贝叶斯（第57-72页）
朴素贝叶斯是贝叶斯定理的经典应用，核心是「属性条件独立假设」，简化计算。

#### 1. 核心思想与公式
- **问题设定**：
  - 实例\(X = <x_1,x_2,...,x_m>\)（\(m\)个属性）；
  - 目标函数\(f(x) \in V\)（有限分类集合，如\(\{yes,no\}\)）。
- **贝叶斯分类逻辑**：
  \[
  y = \arg\max_{v_k \in V} P(v_k|x_1,...,x_m) = \arg\max_{v_k \in V} \frac{P(x_1,...,x_m|v_k)P(v_k)}{P(x_1,...,x_m)}
  \]
  （\(P(x_1,...,x_m)\)为常数，可省略）。
- **朴素假设**：属性条件独立（给定分类\(v_k\)，各属性取值互不影响），即：
  \[
  P(x_1,...,x_m|v_k) = \prod_{j=1}^m P(x_j|v_k)
  \]
- **最终分类公式**：
  \[
  y = \arg\max_{v_k \in V} P(v_k) \prod_{j=1}^m P(x_j|v_k)
  \]

#### 2. 实例：天气数据分类（PlayTennis）
- **训练数据**：14条记录，属性包括Outlook（晴天/阴天/雨天）、Temperature（热/温和/凉）、Humidity（高/正常）、Windy（是/否），分类为P（玩）/N（不玩）。
- **步骤1：计算先验概率**：
  - \(P(yes) = 9/14\)（9条玩的记录）；
  - \(P(no) = 5/14\)（5条不玩的记录）。
- **步骤2：计算属性条件概率**（示例）：
  - Outlook：\(P(sunny|yes)=2/9\)，\(P(sunny|no)=3/5\)；
  - Temperature：\(P(cool|yes)=3/9\)，\(P(cool|no)=1/5\)；
  - 其余属性同理（见文档第59页表格）。
- **步骤3：分类新实例**：
  - 待分类实例：\(<Outlook=sunny, Temp=cool, Hum=high, Wind=strong>\)；
  - 计算加权乘积：
    - \(P(yes) \times P(sunny|yes) \times P(cool|yes) \times P(high|yes) \times P(strong|yes) = 9/14 \times 2/9 \times 3/9 \times 3/9 \times 3/9 ≈ 0.0053\)；
    - \(P(no) \times P(sunny|no) \times P(cool|no) \times P(high|no) \times P(strong|no) = 5/14 \times 3/5 \times 1/5 \times 4/5 \times 3/5 ≈ 0.0206\)；
  - 结论：\(0.0206 > 0.0053\)，分类为\(no\)（不玩）。

#### 3. 文本分类应用
- **建模方式**：
  - 目标分类：\(v_k \in \{like, dislike\}\)；
  - 属性：文档中每个词的位置（词向量表示）；
  - 分类公式：\(y = \arg\max_{v_k} P(v_k) \prod_{j=1}^n P(w_j|v_k)\)（\(n\)为文档词数）。
- **概率估计与平滑**：
  - 原始估计：\(P(w_j|v_k) = \frac{w_j在v_k类文档中的总出现次数}{v_k类文档的总词数}\)；
  - 拉普拉斯平滑（解决“词未出现导致概率为0”的问题）：
    \[
    P(w_j|v_k) = \frac{w_j出现次数 + \alpha}{v_k类总词数 + \alpha \times 词汇表大小}
    \]
    （通常\(\alpha=1\)）。

#### 4. 朴素贝叶斯的变体与问题
- **变体（适配不同数据类型）**：
  - 高斯NB：处理连续特征（用高斯分布建模\(P(x_i|v_k)\)）；
  - 多项式NB：处理多变量离散特征（如词频）；
  - 伯努利NB：处理二元离散/稀疏特征（如词是否出现）。
- **核心问题**：属性条件独立假设过于严格（实际中属性可能相关，如“身高>6英尺”和“体重>200磅”依赖性别）；
- **解决方案**：考虑属性间的依赖关系，引出贝叶斯信念网。


### 四、进阶模型与算法（第73-92页）
涵盖贝叶斯信念网（放松独立假设）、期望最大化（EM）算法（处理隐藏变量）。

#### 1. 贝叶斯信念网（BBNs）
- **核心定位**：放松朴素贝叶斯的“完全独立”假设，允许「子集变量间的条件独立」，结合因果知识和观测数据。
- **定义与结构**：
  - 图形部分：有向无环图（DAG），节点=变量，边=因果关系；
  - 数值部分：条件概率分布（CPDs），每个节点存储「给定父节点时的概率表」。
- **条件独立定义**：给定变量\(Z\)，\(X\)与\(Y\)条件独立 iff \(P(X|Y,Z) = P(X|Z)\)；
  - 示例：给定“闪电”，“雷声”与“下雨”条件独立（\(P(Thunder|Rain,Lightning) = P(Thunder|Lightning)\)）。
- **联合概率计算**：
  \[
  P(y_1,y_2,...,y_n) = \prod_{i=1}^n P(y_i|Parents(Y_i))
  \]
  （每个变量的概率仅依赖其父节点）。
- **应用场景**：医疗诊断（疾病-症状关系）、推荐系统（用户行为-偏好）、风险评估（金融/保险）、故障诊断（机械组件）。
- **优缺点**：
  - 优点：可解释性强（图形结构）、自然处理不确定性和缺失数据、整合先验知识；
  - 挑战：可扩展性差（大型网络推理复杂）、数据稀疏易过拟合（复杂网络参数学习）。

#### 2. 期望最大化（EM）算法
- **核心用途**：处理「数据不完全可观测」的场景（如隐藏变量、缺失值），迭代估计模型参数。
- **适用场景**：无监督聚类（目标值隐藏）、BBN参数训练、隐马尔可夫模型学习。
- **算法步骤**（迭代至收敛）：
  1. **E步（期望步）**：给定当前参数估计，计算隐藏变量的「期望取值」，构造对数似然的期望函数；
  2. **M步（最大化步）**：最大化E步的期望函数，更新参数估计。
- **特性**：每次迭代提升数据的似然度，保证收敛到局部最大值。

#### 3. EM算法实例：硬币投掷实验
- **问题**：有两枚硬币A和B，未知正面概率\(\theta_A,\theta_B\)；5组实验（每组10次投掷），仅知道每组的正反面数（如第1组5H5T），但不知道每组用的是哪枚硬币（隐藏变量\(Z\)）。
- **EM迭代过程**：
  - 初始假设：\(\theta_A^{(0)}=0.6\)，\(\theta_B^{(0)}=0.5\)；
  - E步：计算每组实验属于A/B的概率（如第1组5H5T，\(P(A|5H) = \frac{0.6^5 \times 0.4^5}{0.6^5 \times 0.4^5 + 0.5^{10}} ≈ 0.45\)），得到隐藏变量的期望次数（如第1组属于A的期望H次数≈2.2）；
  - M步：用期望次数更新参数（\(\theta_A^{(1)} = \frac{21.3}{21.3+8.6} ≈ 0.71\)，\(\theta_B^{(1)} = \frac{11.7}{11.7+8.4} ≈ 0.58\)）；
  - 迭代至参数收敛（最终接近真实值\(\theta_A=0.8\)，\(\theta_B=0.45\)）。

#### 4. EM估计高斯混合模型（GMM）
- **GMM生成过程**：随机选择一个高斯分布，按该分布生成实例（数据是多个高斯的混合）。
- **隐藏变量**：\(z_{ij}\)（1表示实例\(x_i\)来自第\(j\)个高斯，0否则）。
- **EM步骤**：
  1. E步：计算\(E[z_{ij}] = \frac{p(x_i|\mu_j)}{p(x_i|\mu_1) + ... + p(x_i|\mu_k)}\)（\(p(x_i|\mu_j)\)是第\(j\)个高斯的密度函数）；
  2. M步：更新高斯均值\(\mu_j = \frac{\sum_{i=1}^m E[z_{ij}]x_i}{\sum_{i=1}^m E[z_{ij}]}\)（加权平均，权重为隐藏变量的期望）。


### 总结
文档按「基础理论→核心方法→经典应用→进阶拓展」的逻辑，系统讲解贝叶斯学习：
1. 以全概率、条件概率为基础，推导贝叶斯定理；
2. 基于贝叶斯定理提出MAP假设和贝叶斯最优分类器，明确两者差异；
3. 通过朴素贝叶斯落地分类任务，利用属性独立假设简化计算；
4. 用贝叶斯信念网放松独立假设，处理复杂变量依赖；
5. 用EM算法解决隐藏变量问题，支撑BBN、GMM等模型的参数学习。

核心主线：**利用概率建模不确定性，通过观测数据更新先验信念，实现预测与分类**。
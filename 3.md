这份文档是同济大学《机器学习》第3章“线性分类”的课件，核心围绕**线性模型家族**展开，从基础的线性回归过渡到分类任务，再到多类分类、类别不平衡解决方案及评价体系，逻辑层层递进。以下按文档顺序（页码对应课件页面）详细拆解内容：


### 一、线性模型概述（第1-2页）
#### 核心定位
线性模型是机器学习中基础且高效的模型族，核心优势是**易理解、易实现、高效可扩展**（easily understood and implemented, efficient and scalable）。

#### 核心子主题
本章涵盖的线性模型分支：
- 线性回归（Linear Regression）：基础回归任务模型
- 线性分类（Linear Classification）：基于线性模型的分类逻辑
- 逻辑斯蒂回归（Logistic Regression）：二分类核心模型
- 线性判别分析（LDA）：基于投影的线性分类方法
- 多类分类（Multi-class Classification）：扩展二分类到多类别场景
- 评价方法（Evaluation Methods）：回归与分类任务的性能度量


### 二、线性回归（第3-10页）
线性回归是后续分类模型的基础，课件先通过具体例子和数学定义铺垫核心逻辑。

#### 1. 模型定义（第3-4页）
- **核心目标**：学习一个线性函数，将输入特征映射到连续输出（回归值）。
- **通用公式**：  
  \( f_{\theta}(x) = \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n + \theta_0 \)  
  其中\( \theta_0 \)是偏置项，\( \theta_1,...,\theta_n \)是特征权重，合称参数\( \theta \)。
- **实例（西瓜打分）**：  
  \( f_{好瓜}(x) = 0.2 \cdot x_{色泽} + 0.5 \cdot x_{根蒂} + 0.3 \cdot x_{敲声} + 1 \)  
  （通过特征加权求和给西瓜“打分”，后续可基于分数分类）

#### 2. 训练数据集与假设空间（第4页）
- 训练集：\( D = \{(x^{(i)}, y^{(i)})\}_{i=1,2,...,N} \)，其中\( x^{(i)} = (x_1^{(i)},...,x_n^{(i)})^T \)是第\( i \)个样本的特征向量，\( y^{(i)} \)是真实标签。
- 假设空间：所有可能的线性函数\( \{f_{\theta}(x^{(i)})\} \)，学习的本质是**更新参数\( \theta \)** ，使预测值\( f_{\theta}(x^{(i)}) \)接近真实标签\( y^{(i)} \)。

#### 3. 代价函数（第5-6页）
- 核心：用**平方损失**衡量预测误差，代价函数（Cost Function）是所有样本损失的平均值：  
  \( J(\theta_0, \theta_1) = \frac{1}{2N} \sum_{i=1}^N (f_{\theta}(x^{(i)}) - y^{(i)})^2 \)  
  （除以2是为了后续求导简化，不影响最优解）
- 性质：平方损失下的代价函数是**凸函数**（Bowled shape），只有唯一最小值，适合梯度下降求解。

#### 4. 求解方法：梯度下降法（第7页）
- 核心逻辑：沿代价函数梯度负方向迭代更新参数，直至收敛。
- 单变量线性回归（\( f_{\theta}(x) = \theta_0 + \theta_1 x \)）的更新规则：  
  \( \theta_1 := \theta_1 - \alpha \cdot \frac{1}{2N} \sum_{i=1}^N (f_{\theta}(x^{(i)}) - y^{(i)}) x^{(i)} \)  
  \( \theta_0 := \theta_0 - \alpha \cdot \frac{1}{2N} \sum_{i=1}^N (f_{\theta}(x^{(i)}) - y^{(i)}) \)  
  其中\( \alpha \)是学习率（步长），需同时更新所有参数。

#### 5. 多变量线性回归（第8-10页）
- 模型扩展：特征数\( n>1 \)，假设函数为\( f_{\theta}(x) = \theta_0 + \theta_1 x_1 + \dots + \theta_n x_n \)。
- 代价函数：\( J(\theta_0, \theta_1,..., \theta_n) = \frac{1}{2N} \sum_{i=1}^N (f_{\theta}(x^{(i)}) - y^{(i)})^2 \)。
- 梯度下降：对每个参数\( \theta_j \)（\( j=0,1,...,n \)），更新规则为：  
  \( \theta_j := \theta_j - \alpha \cdot \frac{1}{N} \sum_{i=1}^N (f_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} \)  
  （\( x_0^{(i)}=1 \)，统一偏置项和权重的更新格式）


### 三、从线性回归到分类：逻辑斯蒂回归（第11-59页）
线性回归输出是连续值，无法直接用于分类（需离散标签0/1），因此引入逻辑斯蒂回归（二分类核心模型）。

#### 1. 分类任务定义（第12-17页）
- 二分类标签：正类（好瓜）=1，负类（坏瓜）=0（或-1）。
- 核心问题：如何将线性回归的连续输出映射到0-1的分类标签？
  - 朴素思路：设定阈值（如0.5），\( \theta^T x \geq 0.5 \)预测1，否则0（\( \theta^T x = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n \)）。
  - 缺陷：线性回归输出无界，阈值选择缺乏理论依据，无法给出分类置信度。

#### 2. Sigmoid函数（第18-21页）
- 解决方案：引入Sigmoid（Logistic）函数，将线性输出映射到[0,1]区间（解释为“正类概率”）：  
  \( f_{\theta}(x) = \frac{1}{1 + e^{-\theta^T x}} \)
- 输出解释：\( f_{\theta}(x) = P(y=1|x;\theta) \)（给定样本x和参数\( \theta \)，y=1的概率），对应的负类概率：  
  \( P(y=0|x;\theta) = 1 - f_{\theta}(x) = \frac{e^{-\theta^T x}}{1 + e^{-\theta^T x}} \)
- 分类规则：仍用阈值0.5，\( f_{\theta}(x) \geq 0.5 \)预测1，否则0（此时\( \theta^T x \geq 0 \)，等价于线性决策边界）。

#### 3. 对数几率的含义（第22-28页）
- 对数几率：事件发生概率与不发生概率的比值的对数，即\( log\left( \frac{P(y=1|x)}{P(y=0|x)} \right) \)。
- 推导：代入Sigmoid函数的概率表达式，可得：  
  \( log\left( \frac{P(y=1|x)}{P(y=0|x)} \right) = \theta^T x \)
- 核心意义：逻辑斯蒂回归本质是**用线性模型拟合“对数几率”** ，实现对分类概率的线性建模。

#### 4. 监督学习的目标与损失函数（第32-49页）
- 学习目标：最小化经验风险（Empirical Risk Minimization, ERM），即所有样本的损失平均值：  
  \( min_{\theta} J(\theta) = min_{\theta} \frac{1}{N} \sum_{i=1}^N L(y^{(i)}, f_{\theta}(x^{(i)})) \)  
  其中\( L(\cdot) \)是损失函数（衡量单样本预测误差）。

- 为什么不用平方损失？  
  若用平方损失\( L = (y - f_{\theta}(x))^2 \)，结合Sigmoid函数后，代价函数\( J(\theta) \)是**非凸函数**（存在多个局部最小值），梯度下降易陷入局部最优。

- 逻辑斯蒂回归的损失函数：交叉熵损失（Cross-entropy Loss）
  - 分段定义：  
    \( L(f_{\theta}(x), y) = \begin{cases} -log(f_{\theta}(x)) & if\ y=1 \\ -log(1 - f_{\theta}(x)) & if\ y=0 \end{cases} \)
  - 统一形式（y∈{0,1}）：  
    \( L(f_{\theta}(x), y) = -y log(f_{\theta}(x)) - (1-y) log(1 - f_{\theta}(x)) \)
  - 直观意义：预测值与真实标签越接近，损失越小；若预测错误（如y=1但f→0），损失趋于无穷大，惩罚力度强。

- 代价函数：\( J(\theta) = \frac{1}{N} \sum_{i=1}^N \left[ -y^{(i)} log(f_{\theta}(x^{(i)})) - (1-y^{(i)}) log(1 - f_{\theta}(x^{(i)})) \right] \)

#### 5. 损失函数的推导：极大似然法（第56-59页）
- 假设：样本独立同分布，每个样本的生成概率为\( P(y^{(i)}|x^{(i)}) = f_{\theta}(x^{(i)})^{y^{(i)}} (1 - f_{\theta}(x^{(i)}))^{1 - y^{(i)}} \)。
- 似然函数（所有样本生成概率的乘积）：\( L(\theta) = \prod_{i=1}^N P(y^{(i)}|x^{(i)}) \)。
- 对数似然：\( log L(\theta) = \sum_{i=1}^N \left[ y^{(i)} log(f_{\theta}(x^{(i)})) + (1-y^{(i)}) log(1 - f_{\theta}(x^{(i)})) \right] \)。
- 极大似然估计：最大化\( log L(\theta) \)等价于最小化其负数，即交叉熵代价函数（与之前定义一致），证明了损失函数的合理性。

#### 6. 求解：梯度下降法（第50-55页）
- 梯度推导：对代价函数\( J(\theta) \)求偏导，最终得到参数更新规则（与线性回归形式一致，但\( f_{\theta}(x) \)是Sigmoid函数）：  
  \( \theta_j := \theta_j - \alpha \cdot \frac{1}{N} \sum_{i=1}^N (f_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} \)
- 关键区别：逻辑斯蒂回归的\( f_{\theta}(x) \)是非线性的，但代价函数是凸函数，梯度下降可找到全局最优解。


### 四、线性判别分析（LDA）（第61-70页）
LDA是另一种线性分类方法，核心思想是**通过投影降维，使同类样本聚合、异类样本分离**（生成模型，与逻辑斯蒂回归的判别模型不同）。

#### 1. 核心目标（第61-64页）
- 投影方向：找到一个向量\( w \)，将高维样本\( x \)投影到一维空间\( z = w^T x \)。
- 优化准则：  
  1. 异类样本的投影均值之差尽可能大；  
  2. 同类样本的投影方差尽可能小。

#### 2. 数学定义（第65-66页）
- 类均值：\( \mu_0 = \frac{1}{N_0} \sum_{x \in X_0} x \)（负类），\( \mu_1 = \frac{1}{N_1} \sum_{x \in X_1} x \)（正类），投影后均值\( \mu_0' = w^T \mu_0 \)，\( \mu_1' = w^T \mu_1 \)。
- 类内方差：投影后负类方差\( Var_0' = w^T \Sigma_0 w \)，正类方差\( Var_1' = w^T \Sigma_1 w \)，其中\( \Sigma_0 = \frac{1}{N_0} \sum_{x \in X_0} (x - \mu_0)(x - \mu_0)^T \)（负类协方差矩阵），\( \Sigma_1 \)同理。
- 优化目标：\( J(w) = \frac{(\mu_0' - \mu_1')^2}{Var_0' + Var_1'} = \frac{w^T (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T w}{w^T (\Sigma_0 + \Sigma_1) w} \)。

#### 3. 关键矩阵与最优解（第67-70页）
- 类间散度矩阵（Between-class Scatter Matrix）：\( S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T \)（衡量异类分离程度）。
- 类内散度矩阵（Within-class Scatter Matrix）：\( S_w = \Sigma_0 + \Sigma_1 \)（衡量同类聚合程度）。
- 优化目标简化：\( J(w) = \frac{w^T S_b w}{w^T S_w w} \)（广义瑞利商）。
- 最优解：通过拉格朗日乘数法求解，得\( w^* = S_w^{-1} (\mu_1 - \mu_0) \)（\( S_w \)可逆时）；若\( S_w \)奇异，可通过SVD分解求逆。


### 五、多类分类（第71-84页）
二分类模型扩展到多类别（如邮件分类：工作/朋友/家庭；天气：晴/阴/雨/雪），核心方法分两类：“二分类器组合”和“直接多类模型”。

#### 1. 二分类器组合策略（第79-84页）
- 一对一（One-vs-One, OvO）：  
  将N个类别两两配对，训练\( N(N-1)/2 \)个二分类器；测试时，所有分类器投票，得票最多的类别为预测结果。
- 一对其余（One-vs-Rest, OvR）：  
  每次将一个类别视为正类，其余所有类别视为负类，训练N个二分类器；测试时，选择预测置信度最高的类别。

#### 2. 直接多类模型：Softmax回归（第72-75页）
- 模型输入：样本特征\( x = (x_1,...,x_n)^T \)。
- 标签编码：用独热向量（One-hot Vector）表示类别，如K=3时，类别$1→(1,0,0)^T$ ，类别$2→(0,1,0)^T$。
- 预测概率：对每个类别k，权重向量为\( \theta_k \)，概率为：  
  \( p_k(x) = \frac{e^{\theta_k^T x}}{\sum_{j=1}^K e^{\theta_j^T x}} \)（Softmax函数，确保所有类别概率和为1）。
- 代价函数（交叉熵）：\( J(\theta) = -\sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} log(p_j^{(i)}) \)，其中\( y_j^{(i)} \)是第i个样本类别j的独热标签。
- 梯度更新：\( \theta_k := \theta_k - \alpha \sum_{i=1}^N (p_k^{(i)} - y_k^{(i)}) x^{(i)} \)。

#### 3. 多类LDA（第75-77页）
- 扩展二分类LDA的散度矩阵：  
  - 类间散度矩阵：\( S_b = \sum_{j=1}^m N_j (\mu_j - \mu)(\mu_j - \mu)^T \)（\( N_j \)为第j类样本数，\( \mu \)为全局均值）。
  - 类内散度矩阵：\( S_w = \sum_{j=1}^m \sum_{x \in X_j} (x - \mu_j)(x - \mu_j)^T \)。
- 优化目标：\( J(W) = \frac{tr(W^T S_b W)}{tr(W^T S_w W)} \)（W是投影矩阵，将样本投影到k维空间，k≤m-1）。
- 求解：通过广义特征值分解\( S_w^{-1} S_b W = W \Lambda \)，取前k个最大特征值对应的特征向量组成W。


### 六、类别不平衡问题（第86页）
#### 问题定义
二分类中两类样本数量差异极大（如正类5个，负类95个），导致模型偏向多数类（如全部预测为负类，准确率达95%但无实际意义）。

#### 解决方案
- 数据层面：欠采样（如EasyEnsemble、BalanceCascade，减少多数类样本）、过采样（如SMOTE、Borderline SMOTE，合成少数类样本）、数据增强。
- 算法层面：类别加权（给少数类样本更高的损失权重）、阈值移动（调整分类阈值，如降低少数类的预测阈值）、集成方法。


### 七、评价指标（第87-108页）
根据任务类型（回归/分类）设计不同评价指标，核心解决“如何衡量模型性能”的问题。

#### 1. 回归任务指标（第87页）
- 均方误差（MSE）：\( MSE = \frac{1}{N} \sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2 \)
- 均方根误差（RMSE）：\( RMSE = \sqrt{MSE} \)（与原数据同量级）
- 平均绝对误差（MAE）：\( MAE = \frac{1}{N} \sum_{i=1}^N |y^{(i)} - \hat{y}^{(i)}| \)
- 决定系数（R²）：衡量模型解释数据变异的能力，R²越接近1越好。

#### 2. 分类任务指标（核心）
##### （1）混淆矩阵（第88页）
二分类混淆矩阵（m×m矩阵扩展到多类）：
| 真实\预测 | 正类（1） | 负类（0） |
|-----------|-----------|-----------|
| 正类（1） | TP（真阳性） | FN（假阴性） |
| 负类（0） | FP（假阳性） | TN（真阴性） |

##### （2）基础指标（第89-92页）
- 准确率（Accuracy）：\( Accuracy = \frac{TP + TN}{TP + TN + FP + FN} \)（适用于样本均衡场景）。
- 错误率（Error Rate）：\( Error Rate = \frac{FP + FN}{TP + TN + FP + FN} = 1 - Accuracy \)。
- 局限性示例：正类5/负类95，全预测为负类时，Accuracy=95%，但模型无价值。

##### （3）查准率、查全率、F度量（第93-98页）
- 查准率（Precision，精确率）：预测为正类的样本中，真实正类的比例：\( Prec = \frac{TP}{TP + FP} \)。
- 查全率（Recall，召回率/敏感度）：真实正类中，被正确预测的比例：\( Rec = \frac{TP}{TP + FN} \)。
- 示例：正类50/负类50，模型仅识别1个正类（TP=1，FN=49，FP=0），则Prec=100%，Rec=2%。
- 权衡关系：阈值越高，Prec越高、Rec越低；阈值越低，Prec越低、Rec越高。
- F度量（综合Prec和Rec）：  
  \( F_1 = \frac{2 \cdot Prec \cdot Rec}{Prec + Rec} \)（Prec和Rec等权）；  
  \( F_{\beta} = \frac{(1+\beta^2) \cdot Prec \cdot Rec}{\beta^2 \cdot Prec + Rec} \)（β>1侧重Rec，β<1侧重Prec）。

##### （4）ROC曲线与AUC（第100-107页）
- 核心指标：  
  - 真阳性率（TPR）：\( TPR = \frac{TP}{TP + FN} = Rec \)（敏感度）；  
  - 假阳性率（FPR）：\( FPR = \frac{FP}{TN + FP} \)（1-特异度）。
- ROC曲线：以FPR为横轴、TPR为纵轴，通过调整分类阈值绘制的曲线，越靠近左上角性能越好。
- AUC（ROC曲线下面积）：衡量模型排序能力，AUC=1为完美模型，AUC=0.5为随机模型。
- 绘制方法：按预测概率降序排序样本，依次将每个样本设为正类，计算每次的FPR和TPR，连接各点得到曲线。

##### （5）PR曲线与PR-AUC（第99-100页）
- 以Recall（查全率）为横轴、Precision（查准率）为纵轴，适用于**类别不平衡场景**（比ROC-AUC更敏感于少数类性能），理想点为(1,1)。

#### 3. 多类分类评价指标（第108页）
- 基础指标：Accuracy（直接扩展）、混淆矩阵（m×m）。
- 扩展指标：Precision、Recall、F1需通过“宏平均”（Macro-averaging，对各类指标取平均）、“微平均”（Micro-averaging，对所有样本的TP/FP/FN汇总计算）或“加权平均”（Weighted-averaging，按类别样本数加权）。
- 特殊指标：Top-k Accuracy（适用于多候选预测场景，前k个预测中包含真实类别即算正确）。


### 总结
本章以“线性模型”为核心，从基础的线性回归出发，逐步延伸到二分类（逻辑斯蒂回归、LDA）、多类分类（Softmax、二分类器组合），再到实际应用中的类别不平衡问题和性能评价体系，形成了“模型定义→数学推导→求解方法→实际问题→性能度量”的完整逻辑链，核心是利用线性关系实现高效的分类任务，同时兼顾理论严谨性和工程实用性。
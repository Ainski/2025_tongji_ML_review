这份文档是同济大学《机器学习》第11章“降维”的教学内容，按页面顺序可分为**维度灾难、降维与特征选择定义、降维方法（重点是PCA、t-SNE、自编码器、ICA、NMF）、特征选择三大方法**四大核心模块，以下逐部分详细拆解：


### 一、维度灾难（Curse of Dimensionality）—— 降维的必要性
文档开篇首先阐明“为什么需要降维”，核心是高维数据带来的一系列问题：
1. **高维数据的核心痛点**：
   - 现实场景中很多数据包含数百到数万个变量，其中大量是冗余或无关特征；
   - 高维空间中数据的概率分布极其复杂，难以估计（如变量间依赖关系）；
   - 冗余/无关特征会干扰模型学习，且训练数据量往往有限、计算资源不足。

2. **空间采样与样本稀疏性**：
   - 举例：若每个维度分4个单元，每个单元10个样本，1维需40个样本，2维需160个样本，10维则需约1000万个样本（\(4^{10}×10\)），高维下样本需求呈指数增长；
   - 样本稀疏性计算：当总样本数为1000时，1维每个单元250个样本，2维62.5个，10维仅0.001个，几乎没有有效样本支撑模型训练。

3. **噪声的致命影响**：
   - 示例：101维特征空间中，正负样本在第1维的真实距离为1，但剩余100维的噪声占比10%；
   - 噪声距离计算：\(\sqrt{100×0.1^2}=1\)，即高维下即使噪声占比低，也会完全掩盖样本的本质差异。


### 二、降维与特征选择的定义区分
这部分明确了“降维”和“特征选择”是两种不同的高维数据处理方式，核心区别在于是否保留原始特征：
1. **降维（Dimensionality Reduction）**：
   - 定义：通过组合现有特征，生成全新的低维特征子集（不是原始特征的挑选，而是重构）；
   - 公式表达：\(\{f_1,...,f_n\} \to \{g_1(f_1,...,f_n),...,g_m(f_1,...,f_n)\}\)，其中\(g_j\)是现有特征的组合函数，\(m < n\)。

2. **特征选择（Feature Selection）**：
   - 定义：从原始特征中直接挑选出子集，不改变特征本身，仅剔除冗余/无关项；
   - 公式表达：\(\{f_1,...,f_n\} \to \{f_{i_1},...,f_{i_m}\}\)，其中\(i_j \in \{1,...,n\}\)，即选中的是原始特征的索引。

3. **机器学习问题分类**：
   - 监督学习：分类（classification）、回归（regression）；
   - 无监督学习：聚类（clustering）、降维（dimensionality reduction），明确降维属于无监督学习范畴。


### 三、降维方法总览与核心方法详解
文档先列出主流降维方法，再重点拆解核心方法（PCA、t-SNE、自编码器、ICA、NMF）：
#### （一）降维方法列表
包含13种常用方法，其中LDA为有监督降维，其余多为无监督：
- 线性类：PCA（主成分分析）、概率PCA、核PCA、LDA（有监督）、CCA（典型相关分析）；
- 非线性类：MDS（多维缩放）、LLE（局部线性嵌入）、LE（拉普拉斯特征映射）、Isomap（等距映射）、t-SNE（t分布随机邻域嵌入）；
- 神经网络类：自编码器（Auto-encoders）、深度自编码器；
- 矩阵分解类：NMF（非负矩阵分解）、ICA（独立成分分析）。

#### （二）降维示例——高维数据的可视化困境
以“65人×53个血液/尿液特征”为例（数据矩阵65×53），说明高维数据的问题：
- 特征间相关性难以观察（53个特征的表格无法直观发现规律）；
- 样本对比困难（高维空间无法可视化不同患者的差异）；
- 多维可视化局限：2维/3维可通过散点图展示，但4维及以上无法直观呈现，引出核心解决方案——PCA。

#### （三）核心方法1：主成分分析（PCA）—— 线性降维核心
1. **PCA的核心目标**：
   - 将高维数据正交投影到低维线性空间，同时满足两个条件：
     1. 最大化投影后数据的方差（保留数据核心信息）；
     2. 最小化数据点与投影点的均方距离（重构误差最小）。

2. **数据准备**：
   - 若特征单位/尺度不同，需先标准化（包括中心化：减去均值；缩放：统一方差），避免某一特征因尺度大主导方差。

3. **三种PCA实现算法**：
   - 算法一：顺序求解（迭代法）
     - 核心：从数据质心出发，第1主成分指向方差最大的方向，后续每个主成分需正交于之前所有成分，且在剩余子空间中方差最大；
     - 公式：第k主成分\(w_k = argmax_{\|w\|=1} \frac{1}{m}\sum_{i=1}^m [w^T(x_i - \sum_{j=1}^{k-1}w_jw_j^Tx_i)]^2\)，可通过梯度上升求解。
   - 算法二：特征值分解（适用于数据维度不高的场景）
     - 步骤：① 数据中心化（每个特征减均值）；② 计算协方差矩阵\(\Sigma = \frac{1}{m}\sum_{i=1}^m (x_i - \bar{x})(x_i - \bar{x})^T\)；③ 对\(\Sigma\)做特征值分解\(\Sigma = U\Lambda U^T\)；④ 按特征值\(\lambda\)降序排序，取前k个特征向量\(U_k\)作为主成分。
   - 算法三：奇异值分解（SVD，适用于任意大小矩阵）
     - 步骤：对中心化后的数据矩阵\(X_{N×m}\)做SVD分解\(X = USV^T\)，近似为\(X \approx U_{N×k}S_{k×k}V_{k×m}^T\)；
     - 关键解读：\(U\)的列是主向量（正交单位向量），\(S\)是对角矩阵（对角线元素为奇异值，体现主成分重要性），\(V^T\)的列是样本重构系数。

4. **PCA的另一个视角——重构误差最小化**：
   - 核心：将中心化数据\(x - \bar{x}\)用k个主成分近似表示\(\hat{x} = c_1u^1 +...+c_ku^k\)，最小化重构误差\(L = \sum\|(x - \bar{x}) - \hat{x}\|_2^2\)；
   - 降维转换：\(Z = Wx\)，其中\(Z\)是低维表示（k维），\(W\)是主成分构成的矩阵（\(k×n\)）。

5. **主成分个数的选择**：
   - 核心原则：累计方差贡献率，即前k个特征值之和占所有特征值总和的比例达到阈值（如85%）；
   - 示例：特征值比例为0.45、0.18、0.13、0.12、0.07、0.04，前4个累计贡献率达88%，选择4个主成分即可。

6. **PCA的应用**：
   - 图像压缩：将372×492图像分割为12×12像素补丁（144维向量），用60个主成分重构，效果类似JPG的离散余弦变换；
   - 噪声过滤：用15个主成分重构含噪图像，过滤噪声；
   - 人脸分析（Eigen-face）：用30个主成分即可重构出清晰的人脸图像。

7. **PCA的弱点**：
   - 无监督学习：不利用标签信息；
   - 线性假设：仅适用于线性可分的高维数据，无法处理非线性结构。

#### （四）核心方法2：t-SNE—— 非线性降维（可视化首选）
1. **核心思想**：
   - 保留高维数据的局部相似性，将高维数据映射到低维空间（常用2维/3维可视化），通过KL散度最小化高维与低维的相似度分布差异。

2. **关键步骤**：
   - 1. 计算高维数据相似度\(P(x^j|x^i)\)：\(P(x^j|x^i) = \frac{S(x^i,x^j)}{\sum_{k≠i}S(x^i,x^k)}\)，其中\(S(x^i,x^j) = exp(-\|x^i - x^j\|_2^2)\)（高斯核）；
   - 2. 计算低维数据相似度\(Q(z^j|z^i)\)：t-SNE用t分布替代SNE的高斯核，\(S'(z^i,z^j) = \frac{1}{1 + \|z^i - z^j\|_2^2}\)，避免SNE的“拥挤问题”；
   - 3. 最小化损失：\(L = \sum_i KL(P(*|x^i) \| Q(*|z^i)) = \sum_{i,j} P(x^j|x^i)log\frac{P(x^j|x^i)}{Q(z^j|z^i)}\)。

#### （五）核心方法3：自编码器（Auto-encoder）—— 神经网络降维
1. **定义与结构**：
   - 无监督学习模型，通过神经网络学习输入数据的紧凑表示（低维编码）；
   - 结构：输入层→编码层→瓶颈层（低维，编码结果）→解码层→输出层，目标是让输出层重构输入层（\(\hat{x} ≈ x\)）。

2. **学习过程**：
   - 编码：输入\(x\)（如784维的MNIST图像）通过编码器（神经网络）得到低维编码\(c\)（维度远小于784）；
   - 解码：编码\(c\)通过解码器重构输入\(\hat{x}\)；
   - 训练目标：最小化重构误差（如MSE）。

3. **与PCA的对比**：
   - PCA是线性模型（隐藏层为线性变换，\(W^TWx\)重构）；
   - 自编码器可通过激活函数（如ReLU）实现非线性变换，能捕捉更复杂的数据结构。

4. **深度自编码器**：
   - 结构：增加编码层和解码层的深度（多层神经网络），用RBM（限制玻尔兹曼机）逐层预训练初始化权重；
   - 优势：重构效果优于PCA（如MNIST图像重构，深度自编码器更清晰）；
   - 应用：去噪声自编码器（Denoising AE）—— 给输入添加噪声，训练模型重构原始无噪数据，实现噪声过滤。

#### （六）核心方法4：非负矩阵分解（NMF）
1. **核心约束**：
   - 将数据矩阵\(X\)分解为\(X ≈ WH\)，其中\(W\)（基矩阵）和\(H\)（系数矩阵）均为非负矩阵（元素≥0）。

2. **优势与意义**：
   - 物理可解释性：非负约束使得分解结果是“加法组合”，而非PCA的“正负抵消”，因此分解出的基向量更像“数据的部分”（如人脸的眼睛、鼻子，数字的笔画）。

3. **目标函数**：
   - 平方误差损失：\(argmin \frac{1}{2}\|X - WH\|^2 = \frac{1}{2}\sum_{i,j}(X_{ij} - WH_{ij})^2\)；
   - KL散度损失：\(argmin J(W,H) = \sum_{ij}(X_{ij}ln\frac{X_{ij}}{WH_{ij}} - X_{ij} + WH_{ij})\)（适用于非负计数数据）。

#### （七）核心方法5：独立成分分析（ICA）—— 盲源分离
1. **问题背景：鸡尾酒会问题**：
   - 观测信号\(x = As\)，其中\(x\)是混合信号（如多个麦克风的录音），\(A\)是未知混合矩阵，\(s\)是未知源信号（如不同人的说话声）；
   - 目标：在未知\(A\)和\(s\)的情况下，从\(x\)中分离出纯净的源信号\(s\)。

2. **ICA的三大假设**：
   - 源信号\(s_1,...,s_K\)统计独立；
   - 源信号中最多一个是高斯分布（非高斯性是独立的关键）；
   - 混合矩阵\(A\)是列满秩矩阵。

3. **核心原则：非高斯性即独立性**：
   - 线性组合\(y = w^T x = w^T A s = z^T s\)，其中\(z = A^T w\)；
   - 关键发现：\(z^T s\)的高斯性比任何单个源信号\(s_i\)都强，当\(y\)最非高斯时，\(z\)仅有一个非零元素，即\(y = s_i\)（分离出单个源信号）。

4. **非高斯性的衡量指标**：
   - 峰度（Kurtosis）：高斯分布峰度为0，亚高斯（如均匀分布）为负，超高斯（如脉冲信号）为正；公式：\(kurt(y) = E\{y^4\} - 3(E\{y^2\})^2\)；
   - 熵（Entropy）：高斯分布的熵最大，熵越小越非高斯；
   - 负熵（Neg-entropy）：\(J(y) = H(y_{gauss}) - H(y)\)，高斯分布负熵为0，可通过近似公式计算（如基于峰度或高斯随机变量的期望）。

5. **ICA的应用**：
   - 图像去噪：比维纳滤波效果更好，能保留图像细节；
   - 视频源分离：从混合视频中分离出独立的源图像。


### 四、特征选择的三大方法
文档最后一部分详细讲解特征选择的三种核心思路，核心目标是找到最优特征子集，提升模型效率和泛化能力：
#### （一）过滤法（Filter Methods）
1. **核心特点**：
   - 独立于后续的学习器（分类器/回归器），属于预处理步骤；
   - 仅基于特征本身的统计属性评估特征重要性，筛选子集。

2. **常用评估指标**：
   - Relief算法：\(\delta^j = \sum_i [-diff(x_i^j, x_{i,nh}^j)^2 + diff(x_i^j, x_{i,nm}^j)^2]\)（通过邻近样本的特征差异评估相关性）；
   - Fischer比率：\(\frac{|S_b|}{|S_w|} = \frac{(\mu_1 - \mu_2)^2}{\sigma_1^2 + \sigma_2^2}\)（类间方差/类内方差，越大特征区分度越强）；
   - 信息增益：\(Gain(A) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)\)（基于决策树的信息熵减少量）；
   - 相关系数：\(\mathfrak{R}(i) = \frac{cov(X_i,Y)}{\sigma(X_i)×\sigma(Y)}\)（特征与标签的线性相关性）。

3. **优势与不足**：
   - 优势：计算高效，不依赖学习器，泛化性强；
   - 不足：未考虑特征间的交互作用，可能筛选出冗余特征。

#### （二）包裹法（Wrapper Methods）
1. **核心特点**：
   - 依赖后续学习器，将学习器的性能（如分类准确率）作为特征子集的评估标准；
   - 把特征选择视为“搜索问题”，通过搜索策略寻找最优子集。

2. **搜索策略**：
   - 前向选择：从空特征集开始，每次添加一个能提升模型性能的特征；
   - 后向消除：从全特征集开始，每次剔除一个对模型性能影响最小的特征；
   - 注：最优特征子集搜索是NP难问题，需用启发式策略（如贪心）简化。

3. **评估方式**：
   - 用验证集或交叉验证（CV）评估特征子集的性能，避免过拟合。

4. **优势与不足**：
   - 优势：考虑特征间交互，筛选出的子集适配特定学习器，性能更优；
   - 不足：计算量大（需反复训练学习器），容易过拟合（过度依赖学习器）。

#### （三）嵌入法（Embedded Methods）
1. **核心特点**：
   - 特征选择与模型训练过程“嵌入”在一起，在训练中隐式完成特征筛选；
   - 依赖特定学习器，通过学习器的正则化或结构约束实现特征选择。

2. **典型例子**：
   - LASSO回归：目标函数\(min_w \sum_{i=1}^m (y_i - w^T x_i)^2 + \lambda\|w\|_1\)，L1正则化会使部分特征的权重\(w_i=0\)，对应特征被剔除；
   - 随机森林/梯度提升：通过特征重要性（如节点分裂增益）自动筛选关键特征，不重要特征的重要性得分接近0。

3. **优势与不足**：
   - 优势：兼顾效率与性能，考虑特征交互，不易过拟合；
   - 不足：与学习器强绑定，通用性差，更换学习器需重新筛选。


### 总结
文档整体逻辑是“为什么降维（维度灾难）→ 降维与特征选择的区别 → 怎么降维（核心方法详解）→ 怎么选特征（三大方法）”，核心围绕无监督降维展开，重点突出PCA（线性）、t-SNE（非线性可视化）、自编码器（神经网络）、ICA（盲源分离）、NMF（可解释性）五大方法，同时补充特征选择的实用技巧，形成“降维+特征选择”的完整高维数据处理体系。
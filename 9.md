这份文档是同济大学倪张凯老师的《机器学习》第9章“集成学习”讲义，按文档页码顺序，从核心定义到具体算法，系统讲解了集成学习的原理、方法与实践，以下是逐部分详细内容梳理：

### 一、集成学习的核心定义（第1-3页）
#### 1. 基本概念
集成学习（Ensemble Learning）的核心思想是：**不依赖单个模型，而是通过某种方式组合多个学习器，以获得比单个模型更优的性能**。  
英文表述为“Multiple Classifier Systems/committee-based learning”，本质是“策略性生成并组合多个学习器，以更好解决特定机器学习问题”。

#### 2. 个体学习器的分类
- 同质学习器（Homogeneous）：所有个体学习器属于同一类型（如均为决策树），称为“基学习器（Base Learner）”；
- 异质学习器（Heterogeneous）：个体学习器属于不同类型（如决策树+神经网络+SVM），称为“组件学习器（Component Learner）”。

#### 3. 基础组件
个体学习器可选用各类基础算法：决策树（DT）、神经网络（NN）、支持向量机（SVM）、K近邻（KNN）等。


### 二、集成学习的示例与关键条件（第4页）
通过3组对比实验，直观说明集成学习的有效性依赖的核心条件：

| 实验场景 | 个体模型表现 | 集成结果 | 结论 |
|----------|--------------|----------|------|
| 场景1    | 3个模型各有不同错误（Model1错Sample3，Model2错Sample1，Model3错Sample2） | 集成后全对（√√√） | 有效集成：个体模型误差低+错误不同（多样性） |
| 场景2    | 3个模型错误完全一致（均错Sample3） | 集成后仍错Sample3 | 无效集成：缺乏多样性 |
| 场景3    | 3个模型各对1个样本（Model1对Sample1，Model2对Sample2，Model3对Sample3） | 集成后全错 | 无效集成：个体模型误差过高 |

**核心结论**：成功的集成必须满足两个条件——① 每个个体学习器的误差率较低；② 个体学习器之间存在“多样性”（即犯不同的错误）。


### 三、集成的多样性：来源与测量（第5-6页）
#### 1. 多样性的来源（如何让个体学习器不同）
- 不同学习器类型：DT、NN、KNN、SVM等；
- 不同训练过程：Bagging的自助采样、Boosting的顺序采样；
- 不同参数：神经网络的隐藏层神经元数、初始权重；
- 不同特征集：随机子空间（Random Subspace）、随机森林的特征随机选择；
- 不同输出表示：输出翻转、输出模糊、纠错输出编码（ECOC）；
- 混合方式：上述多种来源的组合。

#### 2. 多样性的量化测量（二元分类任务）
基于两个学习器\(h_i\)和\(h_j\)的列联表（\(a\)：均预测正确；\(b\)：\(h_i\)对\(h_j\)错；\(c\)：\(h_i\)错\(h_j\)对；\(d\)：均预测错误；\(a+b+c+d=m\)），定义4类测量指标：

| 指标名称 | 公式 | 取值范围 | 核心意义 |
|----------|------|----------|----------|
| 不一致性测量（Disagreement Measure） | \(dis_{ij}=\frac{b+c}{m}\) | [0,1] | 取值越大，多样性越高 |
| 相关系数（Correlation Coefficient） | \(\rho_{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}\) | [-1,1] | 正相关→多样性低，负相关→多样性高 |
| Q统计量（Q-Statistic） | \(Q_{ij}=\frac{ad-bc}{ad+bc}\) | [-1,1] | 绝对值≤相关系数绝对值，趋势一致 |
| Kappa统计量（Kappa-Statistic） | \(\kappa=\frac{p_1-p_2}{1-p_2}\)（\(p_1=\frac{a+d}{m}\)，\(p_2=\frac{(a+b)(a+c)+(c+d)(b+d)}{m^2}\)） | ≥0 | 衡量实际一致性与随机一致性的差异，值越高多样性越低 |


### 四、模型结合的5类核心方法（第7-14页）
集成学习的关键步骤是“组合个体模型的输出”，文档介绍了5类主流方法，涵盖回归、分类场景：

#### 1. 平均法（Averaging）
- 适用场景：回归任务；
- 核心思想：对所有个体模型的输出取算术平均；
- 公式：\(F(x)=\frac{1}{L}\sum_{i=1}^L f_i(x)\)（\(L\)为个体模型数量，\(f_i(x)\)为第\(i\)个模型的输出）。

#### 2. 加权平均法（Weighted Averaging）
- 适用场景：回归/分类任务（分类时为加权投票）；
- 核心思想：给每个个体模型分配权重（性能越好权重越高），加权求和；
- 公式：\(F(x)=\sum_{i=1}^L w_i f_i(x)\)（\(w_i\)为第\(i\)个模型的权重，满足\(\sum w_i=1\)）；
- 特点：类似线性回归/分类，训练集成模型时不更新个体模型。

#### 3. 多层结合法（Multi-Layer）
- 核心思想：用神经网络作为集成模型，将个体模型输出与原始特征结合；
- 结构：
  - 第一层（Layer1）：输入原始数据\(x\)，得到所有个体模型输出\(f_1(x),f_2(x),...,f_L(x)\)；
  - 隐藏层：\(h=\tanh(W_1[f,x]+b_1)\)（将个体模型输出\(f\)与原始特征\(x\)融合作为门控）；
  - 第二层（Layer2）：输出最终结果\(F(x)=\sigma(W_2h+b_2)\)（\(\sigma\)为激活函数）。

#### 4. 树模型结合法（Tree Models）
- 核心思想：用决策树作为集成模型，基于“个体模型输出+原始特征”进行分裂；
- 结构：
  - 根节点/中间节点：按个体模型输出（如\(f_1(x)<a_1\)）或原始特征（如\(x_2<a_3\)）分裂；
  - 叶节点：输出最终分类结果（如\(y=-1\)或\(y=1\)）。

#### 5. 堆叠法（Stacking）—— 通用集成框架
- 核心思想：分“两级学习器”，用第一级模型的输出训练第二级模型，实现更复杂的组合；
- 形式化定义：\(F(x)=g(f_1(x),f_2(x),...,f_L(x))\)（\(g\)为第二级学习器，即元分类器/元回归器）；
- 详细流程：
  1. 输入：原始数据集\(D=\{(x_1,y_1),...,(x_m,y_m)\}\)、第一级学习算法\(L_1,...,L_T\)、第二级学习算法\(L\)；
  2. 训练第一级学习器：对每个\(t=1..T\)，用\(L_t\)在\(D\)上训练个体模型\(h_t\)；
  3. 生成新数据集\(D'\)：对每个样本\(x_i\)，计算所有\(h_t(x_i)\)，得到\(D'=\{(h_1(x_i),h_2(x_i),...,h_T(x_i),y_i)\}_{i=1..m}\)；
  4. 训练第二级学习器：用\(L\)在\(D'\)上训练元模型\(h'\)；
  5. 输出：\(H(x)=h'(h_1(x),...,h_T(x))\)。

#### 补充：其他结合方式
文档还列出了投票法（多数投票、加权多数投票、 plurality voting）、贝叶斯模型平均、分段结合法（RegionBoost）等。


### 五、集成方法的分类：并行与顺序（第15-40页）
按“个体学习器的生成模式”，集成方法分为两大类：**并行方法**（个体学习器独立生成）和**顺序方法**（个体学习器依赖前序模型生成）。

#### （一）并行方法：Bagging与随机森林（第15-23页）
核心特点：所有个体学习器同时训练，通过“样本/特征随机化”引入多样性，最终通过投票/平均组合。

##### 1. Bagging（自举汇聚法，Bootstrap Aggregating）
- 提出者：Breiman（1996）；
- 核心步骤：
  1. 自助采样（Bootstrap Sampling）：给定\(n\)个训练样本\(Z=\{(x_1,y_1),...,(x_n,y_n)\}\)，通过“有放回采样”生成\(B\)个新训练集\(Z^{*1},...,Z^{*B}\)（每个样本被采样到的概率≈63.2%，约37%样本未被采样，称为“袋外数据OOB”）；
  2. 训练个体模型：对每个\(Z^{*b}\)，训练一个基学习器\(\hat{f}^{*b}(x)\)；
  3. 组合输出：回归任务取平均\(\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x)\)，分类任务用投票。
- 验证方式：用袋外数据（OOB）验证每个基学习器的性能，无需额外测试集或交叉验证。

##### 2. 随机森林（Random Forest）
- 提出者：Leo Breiman（2001），是Bagging的改进版；
- 核心改进：在Bagging基础上增加“特征随机化”，实现树的“去相关”；
- 算法步骤：
  1. 对\(b=1..B\)：
     a. 用自助采样生成训练集\(Z^*\)；
     b. 生成随机森林树\(T_b\)：对每个叶节点，从\(p\)个特征中随机选\(m≤p\)个（通常\(m=\sqrt{p}\)）作为分裂候选，选择最优特征和分裂点，分裂至达到最小节点规模（不剪枝）；
  2. 组合输出：回归取平均\(\hat{f}_{rf}^B(x)=\frac{1}{B}\sum_{b=1}^B T_b(x)\)，分类用多数投票\(\hat{C}_{rf}^B(x)=\text{majority vote}\{T_b(x)\}_{b=1}^B\)。
- 优点：
  - 全数据训练，无需预留测试集，OOB数据可验证；
  - 预测精度高，参数少，支持分类和回归；
  - 抗过拟合，无需预处理特征选择。

#### （二）顺序方法：Boosting（第24-40页）
核心特点：个体学习器按顺序生成，后一个模型依赖前一个模型的错误，通过“样本加权”或“残差修正”聚焦难例，最终加权组合。

##### 1. Boosting的核心思想（第24-29页）
- 生成模式：顺序训练，后一个基学习器（弱学习器，性能略优于随机猜测）专门修正前一个的错误；
- 样本处理：通过权重调整，让后续模型重点学习前序模型误分类的样本（即“最具信息的数据点”）；
- 输出组合：加权投票（性能好的基学习器权重更高）；
- 核心优势：可将弱学习器提升为强学习器。

##### 2. AdaBoost（自适应增强，1997）
- 提出者：Yoav Freund & Robert Schapire；
- 输入：数据集\(D=\{(x_1,y_1),...,(x_m,y_m)\}\)、基学习算法\(L\)、训练轮数\(T\)；
- 详细流程：
  1. 初始化权重分布：\(D_1(i)=\frac{1}{m}\)（所有样本权重相等）；
  2. 迭代训练（\(t=1..T\)）：
     a. 用分布\(D_t\)训练基学习器\(h_t\)；
     b. 计算\(h_t\)的误差率：\(\epsilon_t=Pr_{x~D_t,y}[h_t(x)≠y]\)（基于当前样本权重的误分类概率）；
     c. 若\(\epsilon_t>0.5\)（性能差于随机猜测），终止迭代；
     d. 计算\(h_t\)的权重：\(\alpha_t=\frac{1}{2}\ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right)\)（误差越小，权重越高）；
     e. 更新样本权重：\(D_{t+1}(i)=\frac{D_t(i)}{Z_t} \times \begin{cases}exp(-\alpha_t) & h_t(x_i)=y_i \\ exp(\alpha_t) & h_t(x_i)≠y_i\end{cases}\)（\(Z_t\)为归一化因子，确保\(D_{t+1}\)是分布）；
  3. 输出最终模型：\(H(x)=\text{sign}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)\)（加权投票的符号判决）。

##### 3. 梯度提升树（GBDT，Gradient Boosting Decision Tree）
- 核心定位：由“回归决策树（CART）+梯度提升（GB）+收缩（Shrinkage）”组成，基学习器为CART树；
- 别名：GBT、GTB、GBRT、MART，Sklearn中对应`GradientBoostingClassifier`/`GradientBoostingRegressor`；
- 核心思想：前向分步加性模型，每次迭代训练一棵决策树，拟合当前模型的“梯度残差”（即损失函数的负梯度）；
- 数学表达：集成模型为\(f_M(x)=\sum_{m=1}^M T(x,\theta_m)\)（\(T(x,\theta_m)\)为第\(m\)棵CART树，\(\theta_m\)为树参数）；
- 损失函数（以MSE为例）：
  - 损失函数定义：\(L(y,f(x))=(y-f(x))^2\)；
  - 第\(m\)轮迭代：\(f_m(x)=f_{m-1}(x)+T(x,\theta_m)\)，需最小化\(L(y,f_{m-1}(x)+T(x,\theta_m))\)；
  - 残差拟合：\(T(x,\theta_m)\)拟合残差\(r=y-f_{m-1}(x)\)，等价于拟合损失函数的负梯度\(-\left[\frac{\partial L(y,f(x))}{\partial f(x)}\right]_{f(x)=f_{m-1}(x)}\)。
- CART树的损失函数：分裂时最小化左右子树的均方误差和，即\(a_*,v_*=\underset{a\in A}{argmin}\left[min_{c^l}\sum_{x^i\in D^l}(y^i-c^l)^2 + min_{c^r}\sum_{x^i\in D^r}(y^i-c^r)^2\right]\)（\(c^l,c^r\)为左右子树的均值）。
- GBDT vs CART：
  - 预测性能更优：集成学习降低方差和偏差；
  - 捕捉复杂模式：擅长非线性关系；
  - 梯度优化：收敛更快；
  - 抗过拟合：通过弱学习器迭代修正，泛化能力更强。


### 六、梯度提升的优化框架：XGBoost与LightGBM（第41-49页）
#### 1. XGBoost（分布式梯度提升库）
- 核心定位：优化的分布式梯度提升框架，高效、灵活、可移植，是GBDT的工业级实现；
- 核心优势：支持并行树提升，解决大规模数据科学问题的速度和精度领先；
- 目标函数优化：引入正则化项（\(\lambda\)为叶节点权重正则，\(\gamma\)为树复杂度正则），目标函数推导结果为：\(obj^*=-\frac{1}{2}\sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T\)（\(G_j\)为叶节点\(j\)的梯度和，\(H_j\)为海森和）；
- 树分裂策略：贪心选择“增益最大”的分裂点，基于梯度统计（\(g_i,h_i\)）进行节点分裂（如示例中的“age<15”“Is male?”分裂）。

#### 2. LightGBM（轻量级梯度提升机器）
- 核心定位：基于树的梯度提升框架，专为分布式和高效性设计；
- 核心优化（\(LightGBM = XGBoost + GOSS + EFB + 直方图\)）：
  - 速度与内存优化：
    - 梯度单边采样（GOSS）：聚焦梯度大的样本，减少计算量；
    - 互斥特征捆绑（EFB）：将互斥特征捆绑为一个特征，降低维度；
    - 直方图算法：将连续特征离散为直方图，减少内存占用和计算量；
  - 精度优化：叶向生长（Leaf-wise）：优先分裂增益最大的叶节点，而非层向生长，精度更高；
  - 分布式优化：优化网络通信，支持并行、分布式和GPU训练；
- 核心优势：训练速度更快、内存占用更低、精度更高，支持大规模数据。


### 总结
文档从“定义→多样性→模型结合→方法分类→工业级优化”层层递进，完整覆盖集成学习的核心知识：  
1. 集成学习的本质是“组合多样性个体学习器，实现性能提升”；
2. 模型结合方法分为简单组合（平均、投票）和复杂组合（Stacking、树模型）；
3. 集成方法核心分为并行（Bagging/随机森林，强调独立性）和顺序（Boosting，强调依赖性）；
4. 工业界主流实现为XGBoost和LightGBM，通过正则化、采样、特征优化等提升效率和精度。
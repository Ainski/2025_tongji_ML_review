这份文档是同济大学《机器学习》第5章“神经网络”的讲义，围绕神经网络的基础概念、历史发展、核心结构、训练方法及应用特性展开，按文档页面顺序（结合逻辑脉络）详细拆解如下：


### 一、铺垫：传统机器学习方法回顾（第1-4页）
文档开篇通过三种经典机器学习方法引入“从数据到标签的函数学习”核心逻辑，为神经网络铺垫基础：
1. **线性回归（Linear Regression）**
   - 输入：带标签的训练数据集 \( D=\{(x^{(i)}, y^{(i)})\}_{i=1,2,...,N} \)
   - 目标：学习线性函数 \( f_\theta(x) = \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n + \theta_0 \)
   - 核心：通过更新参数 \( \theta \) 使预测值 \( f_\theta(x^{(i)}) \) 接近标签 \( y^{(i)} \)，函数集合 \( \{f_\theta(x^{(i)})\} \) 称为“假设空间”。

2. **逻辑斯蒂回归（Logistic Regression）**
   - 与线性回归框架一致，但引入Sigmoid函数实现概率输出：\( f_\theta(x) = \frac{1}{1+e^{-\theta^T x}} \)
   - 适用于二分类任务，输出值可解释为样本属于正类的概率。

3. **决策树学习（Decision Tree Learning）**
   - 假设空间是“决策树结构”，通过树的分支节点（特征判断）和叶节点（标签输出）实现从数据到标签的映射。
   - 学习本质仍是更新参数（树的分裂规则、节点特征选择），使预测贴近标签。


### 二、动机：高维数据的挑战（第5-7页）
以“计算机视觉-车辆检测”为例，说明传统方法的局限性：
- 相机输入是像素矩阵（50x50灰度图含2500个特征，RGB图含7500个特征），若考虑二次特征，维度可达300万；
- 传统方法难以处理高维特征的复杂映射，而神经网络擅长学习这类非线性、高维数据的函数关系，引出神经网络的必要性。


### 三、生物原型：大脑的神经元与学习机制（第10、12、19、20页）
神经网络的设计灵感源于人脑，文档先介绍生物神经元的结构与工作原理：
1. **大脑神经元的特性**
   - 大脑由约 \( 10^{11} \) 个神经元组成，每个神经元平均连接 \( 10^4 \) 个其他神经元，形成复杂网络；
   - 神经元通过“树突”接收来自其他神经元的输入信号，信号在“细胞体”中求和，当总和超过阈值时，通过“轴突”向其他神经元发送电信号（突触是信号传递的连接点）。

2. **核心假设**：“单一学习算法”假说（[Metin & Frost, 1989]）—— 大脑的体感皮层（Somatosensory Cortex）可通过学习具备“视觉”功能，说明大脑存在通用的学习机制，为人工神经网络的统一框架提供生物依据。


### 四、人工神经网络基础（第13-35页）
#### 1. 定义与分类（第13页）
- 神经网络分为“生物模型”（模拟人脑结构）和“人工模型”（工程实现）；
- 目标：构建能像人脑一样完成复杂计算的人工系统。

#### 2. 历史发展（第14-18页）
文档将神经网络发展分为三波，核心节点如下：
| 发展阶段 | 关键事件 | 核心贡献/影响 |
|----------|----------|---------------|
| 第一波（1943-1969） | 1943：McCulloch-Pitts神经元模型（首次提出神经元的数学抽象）；1958：Rosenblatt提出“感知器”（单层神经网络）；1969：Minsky&Papert指出单层感知器仅能处理线性可分问题 | 奠定基础，但单层局限导致领域停滞 |
| 第二波（1982-1986） | 1982：Hopfield网络（能量基模型）；1984：Hopfield网络解决TSP（NP完全问题）；1986：Rumelhart、Hinton等人重新发现“反向传播算法” | 突破单层局限，多层网络训练可行，领域复苏 |
| 第三波（2006至今） | 2006：深度学习兴起；2012：AlexNet获ImageNet冠军（深度学习重大突破）；2016：AlphaGo击败李世石（4-1） | 大数据+GPU+DNN驱动，应用场景爆发 |

#### 3. 人工神经元结构（第21-22、29、30、34-35页）
人工神经元是神经网络的基本单元，数学模型为：
\[ y = \sigma\left( \sum_{i} w_i x_i - b \right) \]
- 输入 \( x_1, x_2, ..., x_n \)：来自前一层的信号；
- 权重 \( w_i \)：调节每个输入的重要性；
- 偏置 \( b \)：控制神经元的激活阈值；
- 激活函数 \( \sigma(z) \)：引入非线性，使网络能学习复杂函数（核心作用）。

#### 4. 感知器与局限（第23-26页）
- 感知器是“单层人工神经网络”，激活函数为“硬限制器”：
  \[ \sigma(z) = \begin{cases} 1 & z \geq 0 \\ 0 & z < 0 \end{cases} \]
- 训练法则（学习率 \( \eta \) 控制更新步长）：
  \[ w_i = w_i + \eta(y - \hat{y})x_i, \quad b = b + \eta(y - \hat{y}) \]
  - 正确预测时不更新；预测偏高则降低活跃输入的权重；预测偏低则增加活跃输入的权重。
- 局限：仅能学习“线性可分”概念（如能解决AND、OR问题，但无法解决XOR问题），这是第一波神经网络停滞的核心原因。

#### 5. 隐藏层的作用（第27-28页）
- 引入“隐藏层”（输入层与输出层之间的中间层）可突破线性局限：
  - 两层网络（输入层+隐藏层+输出层）可表示所有布尔函数；
  - 示例：通过三个感知器（隐藏层）的线性组合，实现XOR等非线性函数的分类。

#### 6. 激活函数选择（第31-33页）
激活函数需满足“连续、可微、非线性、易计算”四大条件，文档重点介绍Logistic Sigmoid：
- 函数形式：\( \sigma(z) = \frac{1}{1+e^{-z}} \)
- 导数特性：\( \sigma'(z) = \sigma(z)(1 - \sigma(z)) \)（计算简便，适合反向传播）；
- 输出范围：[0,1]，可解释为神经元“激活”的概率（模拟生物神经元的“ firing ”特性）。


### 五、神经网络的结构与函数表示（第36-48页）
#### 1. 标准结构（第36-37页）
神经网络的核心结构包括三层，单元间通过带权重的链接连接：
- 输入层：将原始数据转换为固定长度的数值向量（用户定义维度）；
- 隐藏层：学习中间计算特征，通常仅需1层（即“两层网络”，输入层不计入层数）；
- 输出层：输出固定长度的向量（分类任务输出概率分布，回归任务输出连续值）。

#### 2. 全连接前馈神经网络（第40-45页）
- 定义：每层的每个单元与下一层的所有单元完全连接（“全连接”），信号仅从输入层向输出层传递（“前馈”，无反馈链接）。
- 函数特性：网络结构固定时，权重 \( w \) 和偏置 \( b \) 决定函数映射关系——不同参数对应不同的从输入到输出的函数。
- 示例：
  - 输入向量 \( \begin{bmatrix} 1 \\ -1 \end{bmatrix} \) 输出 \( \begin{bmatrix} 0.62 \\ 0.83 \end{bmatrix} \)；
  - 输入向量 \( \begin{bmatrix} 0 \\ 0 \end{bmatrix} \) 输出 \( \begin{bmatrix} 0.51 \\ 0.85 \end{bmatrix} \)。


### 六、训练核心：损失函数与梯度下降（第46-49页）
#### 1. 学习目标（第46页）
- 目标：经验风险最小化（Empirical Risk Minimization, ERM），即最小化所有样本的平均损失：
  \[ \min_\theta \frac{1}{N} \sum_{i=1}^N L\left(y^{(i)}, f_\theta(x^{(i)})\right) \]
  - \( L(\cdot) \) 为损失函数，衡量单个样本的预测误差；
  - \( \theta \) 为网络所有参数（权重 \( w \) + 偏置 \( b \)）。

#### 2. 常用损失函数（第47页）
| 损失函数类型 | 公式 | 适用场景 |
|--------------|------|----------|
| 0-1损失 | \( L(y, \hat{y}) = \begin{cases} 1 & y \neq \hat{y} \\ 0 & y = \hat{y} \end{cases} \) | 分类任务（仅判断对错，不考虑误差大小） |
| 均方误差（MSE） | \( L(y, \hat{y}) = (y - \hat{y})^2 \) | 回归任务（衡量连续值的误差） |
| 绝对损失 | $L(y,\hat{y})=\|y-\hat{y}\|$ | 回归任务（对异常值更鲁棒） |
| 交叉熵损失 | \( L(y, p) = -[y\log p + (1-y)\log(1-p)] \) | 二分类任务（输出为概率时） |
| 多分类交叉熵 | \( L(y, p) = -\sum_j y_j \log p_j \) | 多分类任务（结合Softmax输出概率分布） |

#### 3. 梯度下降求解（第49页）
- 核心思想：沿损失函数的梯度方向更新参数，逐步减小损失：
  \[ \theta^{t+1} = \theta^t - \eta \nabla L(\theta^t) \]
  - \( \eta \) 为学习率，\( \nabla L(\theta) \) 为损失函数对所有参数的梯度向量。
- 问题：神经网络参数数量庞大（数百万级），直接计算梯度效率极低，需通过“反向传播算法”高效求解梯度。


### 七、核心算法：反向传播（Backpropagation）（第50-80页）
反向传播是多层神经网络的核心训练算法，通过“正向传递+反向传递”高效计算梯度，文档分步骤详细讲解：

#### 1. 前提：Softmax回归（多分类场景）（第52-54页）
- 适用：多分类任务，输出层采用Softmax激活，将网络输出转换为概率分布：
  - 输入：\( x = (x_1, x_2, ..., x_n)^T \)；
  - 输出：类别标签用One-hot向量表示（如3类任务中，类别1为 \( (1,0,0)^T \)）；
  - 预测概率：对第 \( k \) 类，\( p_k = \frac{e^{\theta_k^T x}}{\sum_{j=1}^K e^{\theta_j^T x}} \)（\( \theta_k \) 为第 \( k \) 类的权重向量）；
  - 损失函数：多分类交叉熵 \( L(y, p) = -\sum_{j=1}^K y_j \log p_j \)；
  - 预测标签：\( \hat{y} = \arg\max_j p_j(x) \)（选择概率最大的类别）。

#### 2. 反向传播的核心逻辑（第55-63页）
基于“链式法则”，分两步计算梯度：
- **正向传递（Forward Pass）**：计算所有神经元的输入 \( z \) 和输出 \( a = \sigma(z) \)，并记录 \( \frac{\partial z}{\partial w} = x \)（权重 \( w \) 连接的输入值）。
- **反向传递（Backward Pass）**：从输出层反向递归计算 \( \frac{\partial C}{\partial z} \)（损失对神经元输入 \( z \) 的梯度），再通过 \( \frac{\partial C}{\partial w} = \frac{\partial C}{\partial z} \cdot \frac{\partial z}{\partial w} \) 得到权重的梯度。

#### 3. 具体计算步骤（第64-79页）
- **输出层梯度**（直接计算）：
  - 回归任务（MSE损失）：\( \frac{\partial C}{\partial z_i} = (a_i - y_i) \sigma'(z_i) \)；
  - 多分类任务（Softmax+交叉熵）：\( \frac{\partial C}{\partial z_i} = a_i - y_i \)（简化结果，无需计算Sigmoid导数，避免梯度消失）。
- **隐藏层梯度**（递归计算）：
  对第 \( l \) 层的神经元 \( i \)，梯度为下一层所有神经元梯度的加权和（权重为当前层到下一层的连接权重）：
  \[ \frac{\partial C}{\partial z_i^{(l)}} = \sigma'(z_i^{(l)}) \sum_{j} w_{ji}^{(l+1)} \frac{\partial C}{\partial z_j^{(l+1)}} \]

#### 4. 关键结论（第78-79页）
- MSE损失下，\( \frac{\partial L}{\partial y_i'} = y_i' - y_i \)；
- Softmax+交叉熵损失下，\( \frac{\partial L}{\partial z_i} = y_i' - y_i \)（梯度计算简洁，是多分类任务的首选组合）。


### 八、神经网络的特性与实践（第81-88页）
#### 1. 反向传播的评价（第81页）
- 优点：强大的表征能力——足够多的隐藏单元可学习任何函数；
- 缺点：仅保证收敛到“局部最小值”，而非全局最小值；
- 改进方法：引入动量项、随机梯度下降（SGD）、训练多个网络取最优。

#### 2. 表征能力（第82页）
神经网络的表征能力由层数和隐藏单元数量决定：
- 两层网络：可表示所有布尔函数，逼近任意有界连续函数；
- 三层网络：可逼近任意函数（任意精度）。

#### 3. 神经网络的建立（第83页）
需决策三个核心超参数：
- 单元数量：隐藏单元过少→无法学习复杂函数；过多→泛化能力差（过拟合）；
- 单元类型：选择合适的激活函数（如Sigmoid、ReLU等）；
- 连接方式：全连接、部分连接（如卷积网络）等。

#### 4. 隐藏单元的表征意义（第85-88页）
- 隐藏单元是网络“学习到的新特征”，使目标概念在变换后的特征空间中线性可分；
- 可解释性：部分隐藏单元对应现实意义的特征（如边缘检测器、元音检测器）；部分为“分布式表征”（单个单元无明确意义，组合起来实现特征映射）；
- 示例：8输入→3隐藏→8输出的网络，训练后隐藏层能学习到输入的有效编码，实现“输入=输出”的恒等映射。


### 九、小结（第90页）
1. 神经网络是模拟人脑特性的计算模型，单元的连接方式和性质决定其行为；
2. 感知器是单层前馈网络，仅能处理线性可分问题；
3. 多层前馈网络（含隐藏层）具备强大表征能力，足够单元可学习任何函数；
4. 反向传播是多层网络的核心训练算法，高效计算梯度；
5. 神经网络广泛应用于各类人工学习系统（计算机视觉、语音识别等）。